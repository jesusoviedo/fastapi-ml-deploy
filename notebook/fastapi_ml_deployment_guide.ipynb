{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7abe0e3c-fb18-4f8a-a8a8-7d3fd367cc3a",
   "metadata": {},
   "source": [
    "# Implementación de Modelos de ML con FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215d90e-da16-4b5a-9792-c5c1afcde39f",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Este notebook educativo completo te guía paso a paso en la implementación de modelos de Machine Learning usando las herramientas más modernas de 2025: **FastAPI**, **uv**, **Docker**, y **Fly.io**. Aprenderás a crear un servicio web robusto para servir modelos de ML en producción.\n",
    "\n",
    "### ¿Qué aprenderás?\n",
    "\n",
    "- Configuración moderna de proyectos con **uv** (la alternativa rápida a pip/pipenv)\n",
    "- Entrenamiento y guardado de modelos con **scikit-learn pipelines**\n",
    "- Creación de APIs robustas con **FastAPI**\n",
    "- Validación de datos con **Pydantic**\n",
    "- Contenedorización con **Docker**\n",
    "- Despliegue en la nube con **Fly.io**\n",
    "\n",
    "### Caso de Uso: Predicción de Churn de Clientes\n",
    "\n",
    "Implementaremos un modelo para predecir si un cliente cancelará su servicio (churn), un problema común en telecomunicaciones y servicios de suscripción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8cc97-9d65-45e9-a1a9-251c65114bfa",
   "metadata": {},
   "source": [
    "## 1. Configuración del Entorno con UV\n",
    "\n",
    "### ¿Qué es UV y por qué es tan rápido?\n",
    "\n",
    "Imagina que estás construyendo algo con bloques de LEGO. `pip` es como un ayudante que va a la tienda a por cada pieza que necesitas, una por una. Si una pieza necesita otra más pequeña, tiene que volver a la tienda. Es fiable, pero puede llevar su tiempo.\n",
    "\n",
    "**UV**, en cambio, es como un ayudante con un dron súper-rápido y una tablet. Antes de salir, mira tu lista, calcula al instante todas las piezas y sub-piezas que necesitarás, y las recoge todas de la tienda en un solo viaje a máxima velocidad.\n",
    "\n",
    "En resumen, **UV es un instalador y gestor de entornos virtuales para Python, diseñado para ser extremadamente rápido**. Su objetivo es reemplazar a herramientas como `pip`, `pip-tools`, `venv` y `virtualenv` con una única interfaz de línea de comandos ultrarrápida.\n",
    "\n",
    "### El Secreto de su Velocidad\n",
    "\n",
    "La \"magia\" de UV no es una sola cosa, sino la combinación de tres factores clave:\n",
    "\n",
    "1.  **Está escrito en Rust**: A diferencia de `pip` que está escrito en Python, UV está construido con Rust. Rust es un lenguaje de programación que compila a código máquina nativo, lo que le permite ejecutar tareas como la descarga e instalación de archivos a una velocidad mucho mayor que un lenguaje interpretado como Python. ¡Es como comparar un coche de Fórmula 1 (Rust) con un coche de calle (Python) para una carrera de velocidad!\n",
    "\n",
    "2.  **Resolución de dependencias de última generación**: Cuando instalas un paquete (ej. `pandas`), este depende de otros (ej. `numpy`), que a su vez dependen de otros. Encontrar las versiones correctas que sean compatibles entre sí es un rompecabezas complejo. UV utiliza un algoritmo muy avanzado para resolver este \"puzzle\" de dependencias de forma increíblemente eficiente.\n",
    "\n",
    "3.  **Un sistema de caché global e inteligente**: La primera vez que UV descarga un paquete, lo guarda en una caché global en tu sistema. La próxima vez que necesites ese mismo paquete en *otro proyecto*, UV no lo descarga de nuevo. Simplemente crea un enlace a la versión que ya tiene guardada. Esto hace que la creación de nuevos entornos sea casi instantánea.\n",
    "\n",
    "> **Dato curioso**: El creador de UV, Charlie Marsh, es también el creador de **Ruff**, un *linter* de Python también escrito en Rust que es cientos de veces más rápido que sus predecesores.\n",
    "\n",
    "### UV vs. Pip y otras herramientas\n",
    "\n",
    "Pensar que UV es solo \"un pip más rápido\" es quedarse corto. La verdadera revolución es que **UV es una navaja suiza que reemplaza a un conjunto de herramientas**.\n",
    "\n",
    "La forma tradicional de trabajar en Python requiere un equipo de varias herramientas:\n",
    "* `venv` o `virtualenv`: Para crear y gestionar entornos virtuales aislados.\n",
    "* `pip`: Para instalar los paquetes dentro de ese entorno.\n",
    "* `pip-tools`: Una herramienta extra para compilar un `requirements.txt` a partir de un `pyproject.toml` y generar un archivo de bloqueo (`.txt`) que asegure la reproducibilidad.\n",
    "\n",
    "UV integra todas estas funciones (y más) en un único ejecutable súper rápido.\n",
    "\n",
    "Pensemos en una analogía: `pip` + `venv` es como tener una caja de herramientas con un martillo, un destornillador y una llave inglesa. Funcionan bien, pero tienes que ir cambiando de herramienta para cada tarea. **UV es como una multiherramienta Leatherman de última generación**: tienes todo lo que necesitas en un solo lugar, es más ligera y mucho más eficiente. \n",
    "\n",
    "### Tabla Comparativa Rápida\n",
    "\n",
    "| Característica | `pip` + `venv` | `uv` |\n",
    "| :--- | :--- | :--- |\n",
    "| **Velocidad** | Moderada. La resolución de dependencias puede ser lenta. | **Extremadamente Rápida**. Gracias a Rust y su resolutor avanzado. |\n",
    "| **Herramientas** | Múltiples (`python -m venv`, `pip`). | **Única y unificada** (un solo comando `uv`). |\n",
    "| **Crear Entorno** | `python -m venv .venv` | `uv venv` (notablemente más rápido). |\n",
    "| **Instalación** | `pip install pandas` | `uv pip install pandas` (sintaxis familiar). |\n",
    "| **Caché de Paquetes**| El caché de pip es bueno, pero a veces inconsistente. | **Caché global e inteligente**. Acelera la creación de nuevos proyectos. |\n",
    "| **Reproducibilidad**| Se necesita `pip-tools` para crear un archivo `.txt` de bloqueo. | **Soporte nativo**. Puede leer y generar archivos de bloqueo (`uv.lock`, `requirements.lock`). |\n",
    "\n",
    "> La conclusión es simple: pasas de hacer malabares con 2 o 3 comandos a usar uno solo que, además, es entre **10 y 100 veces más rápido**. En entornos de Integración Continua (CI/CD), donde se crean y destruyen entornos constantemente, este ahorro de tiempo es gigantesco.\n",
    "\n",
    "\n",
    "### Instalación y Configuración\n",
    "\n",
    "```bash\n",
    "# En tu terminal, instala uv (solo una vez)\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Crear nuevo proyecto\n",
    "uv init mi-proyecto-ml\n",
    "cd mi-proyecto-ml\n",
    "```\n",
    "\n",
    "> Otra alternativa: para usar `uv`, necesitas instalarlo en tu sistema. La forma más fácil es con `pip` normal: `pip install uv`).\n",
    "\n",
    "### Primeros Pasos con UV\n",
    "\n",
    "Aquí tienes el flujo de trabajo típico para un nuevo proyecto, paso a paso.\n",
    "\n",
    "#### Paso 1: Crear el Entorno Virtual\n",
    "\n",
    "Olvida el `python -m venv .venv`. Con UV, es más corto y mucho más rápido:\n",
    "\n",
    "```bash\n",
    "# Crear nuevo proyecto\n",
    "uv init mi-proyecto-ml\n",
    "cd mi-proyecto-ml\n",
    "\n",
    "# Crea un entorno virtual en una carpeta llamada .venv\n",
    "uv venv\n",
    "```\n",
    "\n",
    "¡Listo\\! En una fracción de segundo, tendrás tu entorno creado. Si quisieras usar una versión específica de Python que tengas instalada, podrías hacer `uv venv -p 3.11`.\n",
    "\n",
    "#### Paso 2: Activar el Entorno\n",
    "\n",
    "Esta parte es **exactamente igual** a como siempre lo has hecho. UV crea una estructura de carpetas compatible.\n",
    "\n",
    "```bash\n",
    "# En Linux o macOS\n",
    "source .venv/bin/activate\n",
    "\n",
    "# En Windows (Command Prompt)\n",
    ".venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "Una vez activado, tu terminal te mostrará `(.venv)` al principio de la línea.\n",
    "\n",
    "#### Paso 3: Instalar Paquetes\n",
    "\n",
    "La sintaxis es idéntica a la de `pip`, lo cual facilita enormemente la transición. Simplemente reemplazas `pip install` por `uv pip install`, otro comando valido es `uv add`.\n",
    "\n",
    "```bash\n",
    "# Instalar un solo paquete\n",
    "uv pip install fastapi\n",
    "\n",
    "# Instalar varios paquetes a la vez\n",
    "uv pip install \"pandas~=2.0\" pydantic\n",
    "\n",
    "# Instalar desde tu pyproject.toml\n",
    "uv pip install -e .\n",
    "```\n",
    "\n",
    "Aquí notarás la diferencia más grande: la velocidad de descarga e instalación es asombrosa.\n",
    "\n",
    "#### Paso 4: Generar un Archivo de Bloqueo\n",
    "\n",
    "Este es el paso que garantiza que el entorno de todo tu equipo sea idéntico. `uv` lee tu `pyproject.toml` y genera un archivo `requirements.lock` con las versiones exactas de cada paquete.\n",
    "\n",
    "```bash\n",
    "# Lee pyproject.toml y crea un archivo de bloqueo\n",
    "uv pip compile pyproject.toml -o requirements.lock\n",
    "```\n",
    "\n",
    "Este archivo `requirements.lock` es el que subirías a tu repositorio de Git.\n",
    "\n",
    "#### Paso 5: Instalar desde el Archivo de Bloqueo\n",
    "\n",
    "Ahora, imagina que eres un nuevo desarrollador que se une al proyecto. Tienes el `pyproject.toml` y el `requirements.lock`. Después de crear y activar tu entorno, solo necesitas un comando:\n",
    "\n",
    "```bash\n",
    "# Lee el archivo de bloqueo y sincroniza tu entorno.\n",
    "# ¡Instala, elimina y actualiza paquetes para que coincida 100%!\n",
    "uv sync requirements.lock\n",
    "```\n",
    "\n",
    "Este comando es increíblemente rápido y eficiente. Es el que usarías en tus flujos de CI/CD o para que un compañero se ponga al día.\n",
    "\n",
    "### Estructura de Proyecto para ML\n",
    "\n",
    "Esta es la estructura de directorios recomendada para el proyecto, siguiendo las mejores prácticas de desarrollo de software y MLOps.\n",
    "\n",
    "```\n",
    "mi-proyecto-ml/\n",
    "├── .env                  # Variables de entorno y secretos\n",
    "├── .gitignore\n",
    "├── .python-version       # Versión de Python fijada para el proyecto\n",
    "├── pyproject.toml        # Definición de dependencias y configuración\n",
    "├── uv.lock               # Archivo de bloqueo para reproducibilidad\n",
    "├── README.md\n",
    "├── Dockerfile            # Instrucciones para la contenedorización\n",
    "│\n",
    "├── artifacts/            # Modelos entrenados, serializadores y otros artefactos\n",
    "│   └── sentiment_model_v1.pkl\n",
    "│\n",
    "├── data/                 # Datasets del proyecto (ignorado por Git)\n",
    "│   ├── raw/              # Datos originales, sin modificar\n",
    "│   └── processed/        # Datos limpios y listos para el entrenamiento\n",
    "│\n",
    "├── notebooks/            # Jupyter Notebooks para exploración y análisis\n",
    "│   └── 1.0-eda-initial-exploration.ipynb\n",
    "│\n",
    "├── src/                  # Código fuente de la aplicación\n",
    "│   ├── __init__.py\n",
    "│   ├── main.py           # Punto de entrada de la API (FastAPI)\n",
    "│   ├── config.py         # Módulo de configuración\n",
    "│   ├── api/              # Lógica de la API (endpoints)\n",
    "│   ├── ml/               # Código de Machine Learning\n",
    "│   └── schemas/          # Esquemas de datos (Pydantic)\n",
    "│\n",
    "├── tests/                # Pruebas automáticas\n",
    "└── scripts/              # Scripts de utilidad (ej. para descargar datos)\n",
    "```\n",
    "\n",
    "### Explicación de la Estructura\n",
    "\n",
    "La organización de este proyecto está diseñada para ser **clara, modular y escalable**. Cada directorio tiene una responsabilidad bien definida:\n",
    "\n",
    "  * **Configuración (Raíz)**: Los archivos en la raíz del proyecto (`pyproject.toml`, `uv.lock`, `.python-version`, etc.) definen el entorno, las dependencias y las reglas del proyecto, asegurando que cualquier colaborador pueda replicar el entorno de desarrollo de forma idéntica.\n",
    "\n",
    "  * **`src/` (Código Fuente)**: Es el corazón de la aplicación. Contiene todo el código Python que se ejecuta como parte del servicio final. La lógica está modularizada en subpaquetes como `api/`, `ml/` y `schemas/` para mantener el código organizado y fácil de mantener.\n",
    "\n",
    "  * **`artifacts/` (Artefactos)**: Esta carpeta almacena los **productos generados por nuestro código**, no el código en sí. Su principal contenido son los modelos ya entrenados (ej. un archivo `.pkl` o `.h5`).\n",
    "\n",
    "  * **`data/` (Datos)**: Un lugar centralizado para todos los datos necesarios. Se divide en `raw` para los datos originales e inmutables y `processed` para las versiones limpias y transformadas, listas para ser usadas en el entrenamiento. Esta carpeta se añade al `.gitignore` para evitar subir grandes volúmenes de datos al repositorio.\n",
    "\n",
    "  * **`notebooks/` (Experimentación)**: Este es el \"laboratorio\". Contiene los Jupyter Notebooks usados para el Análisis Exploratorio de Datos (EDA), prototipado de modelos y visualizaciones. Separar los notebooks del código de producción en `src/` es crucial para mantener el proyecto limpio.\n",
    "\n",
    "  * **`tests/` y `scripts/` (Soporte)**: `tests/` asegura la calidad y fiabilidad de nuestro código mediante pruebas automáticas, mientras que `scripts/` nos proporciona un lugar para herramientas de un solo uso que facilitan tareas de desarrollo.\n",
    "\n",
    "Esta separación de responsabilidades hace que el proyecto sea más fácil de entender, probar, y finalmente, desplegar a producción.\n",
    "\n",
    "\n",
    "### Configuración de Dependencias (pyproject.toml)\n",
    "\n",
    "`pyproject.toml` es el cerebro detrás de la gestión de proyectos modernos en Python, y herramientas como **UV** están diseñadas para leerlo a la perfección.\n",
    "\n",
    "Piensa en `pyproject.toml` como el **carné de identidad y el panel de control** de tu proyecto, todo en un único archivo.\n",
    "\n",
    "Antes, la configuración de un proyecto de Python estaba repartida en varios archivos: `setup.py`, `requirements.txt`, `setup.cfg`, `MANIFEST.in`... ¡Era un poco caótico\\! El archivo `pyproject.toml` fue introducido (en el [PEP 518](https://peps.python.org/pep-0518/)) para estandarizar y centralizar toda esa información en un solo lugar.\n",
    "\n",
    "### ¿Qué hay dentro de un `pyproject.toml`?\n",
    "\n",
    "Este archivo utiliza el formato [TOML](https://www.google.com/search?q=https://toml.io/es/) (Tom's Obvious, Minimal Language), que es muy fácil de leer para los humanos. Se organiza en secciones, pero nos centraremos en las más importantes para las dependencias.\n",
    "\n",
    "Veamos un ejemplo práctico:\n",
    "\n",
    "```toml\n",
    "# Esta sección le dice a Python CÓMO construir tu proyecto.\n",
    "# No necesitas preocuparte mucho por ella al principio.\n",
    "[build-system]\n",
    "requires = [\"setuptools>=61.0\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "# --- Aquí empieza lo interesante ---\n",
    "\n",
    "# Esta es la \"ficha de identidad\" de tu proyecto.\n",
    "[project]\n",
    "name = \"mi-proyecto-genial\"\n",
    "version = \"0.1.0\"\n",
    "authors = [\n",
    "  { name=\"Tu Nombre\", email=\"tu@email.com\" },\n",
    "]\n",
    "description = \"Un pequeño proyecto de ejemplo.\"\n",
    "\n",
    "# Aquí declaras las dependencias PRINCIPALES.\n",
    "# Estas son las que se necesitan para que tu programa funcione.\n",
    "dependencies = [\n",
    "    \"fastapi>=0.90.0\", # Necesitamos fastapi, versión 0.90.0 o superior.\n",
    "    \"pandas\",         # La última versión estable de pandas.\n",
    "]\n",
    "\n",
    "# Dependencias OPCIONALES. No son necesarias para todos los usuarios.\n",
    "[project.optional-dependencies]\n",
    "test = [\n",
    "    \"pytest\",\n",
    "    \"pytest-cov\",\n",
    "]\n",
    "docs = [\n",
    "    \"sphinx\",\n",
    "]\n",
    "\n",
    "# En esta sección, otras herramientas pueden guardar su configuración.\n",
    "# Por ejemplo, Ruff (el linter del que hablamos) se configura aquí.\n",
    "[tool.ruff]\n",
    "line-length = 88\n",
    "```\n",
    "\n",
    "Las dos secciones clave son:\n",
    "\n",
    "1.  `[project.dependencies]`: Esta es tu lista principal de \"ingredientes\". Es el equivalente moderno al archivo `requirements.txt`. Aquí pones los paquetes que tu proyecto **necesita** para funcionar.\n",
    "2.  `[project.optional-dependencies]`: Aquí defines grupos de dependencias para situaciones específicas. El caso más común es `test` (para instalar librerías de testing como `pytest`) o `dev` (para herramientas de desarrollo). Esto es genial porque alguien que solo quiere *usar* tu programa no necesita descargar todas las herramientas que tú usaste para *crearlo*.\n",
    "\n",
    "### ¿Y cómo se relaciona esto con UV?\n",
    "\n",
    "Aquí es donde todo encaja. **UV está diseñado para leer este archivo de forma nativa y ultrarrápida**.\n",
    "\n",
    "  - Si ejecutas `uv pip install -e .` en la carpeta de tu proyecto, UV leerá la lista de `[project.dependencies]` y las instalará.\n",
    "  - Si quieres instalar también las dependencias de testing, ejecutarías `uv pip install -e \".[test]\"`. UV entenderá que debe instalar las dependencias principales **Y** las del grupo `test`.\n",
    "\n",
    "Usar `pyproject.toml` centraliza toda la configuración, haciendo tu proyecto más limpio, reproducible y fácil de entender tanto para otros desarrolladores como para herramientas automáticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca28df-73a6-4b9d-9052-02fe3c079616",
   "metadata": {},
   "source": [
    "## 2. Generación de Datos Sintéticos para Churn\n",
    "\n",
    "### Crear Dataset de Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7eb78d-00ff-49ca-8648-dbd2368b4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def create_churn_dataset(n_samples=10000, random_state=42):\n",
    "    \"\"\"\n",
    "    Crear un dataset sintético realista para predicción de churn.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con características de clientes y etiquetas de churn\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Crear datos base\n",
    "    data = {\n",
    "        'customer_id': range(1, n_samples + 1),\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "        'senior_citizen': np.random.choice([0, 1], n_samples, p=[0.84, 0.16]),\n",
    "        'partner': np.random.choice(['Yes', 'No'], n_samples, p=[0.48, 0.52]),\n",
    "        'dependents': np.random.choice(['Yes', 'No'], n_samples, p=[0.30, 0.70]),\n",
    "        'tenure': np.random.randint(1, 73, n_samples),  # Meses\n",
    "        'phone_service': np.random.choice(['Yes', 'No'], n_samples, p=[0.90, 0.10]),\n",
    "        'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], \n",
    "                                           n_samples, p=[0.34, 0.44, 0.22]),\n",
    "        'online_security': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                          n_samples, p=[0.29, 0.50, 0.21]),\n",
    "        'tech_support': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                       n_samples, p=[0.29, 0.50, 0.21]),\n",
    "        'contract': np.random.choice(['Month-to-month', 'One year', 'Two year'], \n",
    "                                   n_samples, p=[0.55, 0.21, 0.24]),\n",
    "        'payment_method': np.random.choice([\n",
    "            'Electronic check', 'Mailed check', 'Bank transfer (automatic)', \n",
    "            'Credit card (automatic)'\n",
    "        ], n_samples, p=[0.34, 0.23, 0.22, 0.21]),\n",
    "        'monthly_charges': np.round(np.random.normal(64.76, 30.0, n_samples), 2),\n",
    "        'total_charges': np.round(np.random.normal(2283.30, 2266.77, n_samples), 2)\n",
    "    }\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Limpiar valores negativos en charges\n",
    "    df['monthly_charges'] = df['monthly_charges'].clip(lower=18.25)\n",
    "    df['total_charges'] = df['total_charges'].clip(lower=18.80)\n",
    "    \n",
    "    # Crear etiquetas de churn con lógica realista\n",
    "    churn_probability = 0.2  # Baseline\n",
    "    \n",
    "    # Factores que aumentan churn\n",
    "    tenure_factor = np.where(df['tenure'] < 12, 0.15, 0)  # Clientes nuevos\n",
    "    contract_factor = np.where(df['contract'] == 'Month-to-month', 0.20, 0)  # Sin compromiso\n",
    "    payment_factor = np.where(df['payment_method'] == 'Electronic check', 0.10, 0)  # Método de pago\n",
    "    charges_factor = np.where(df['monthly_charges'] > 80, 0.08, 0)  # Cargos altos\n",
    "    \n",
    "    # Factores que reducen churn\n",
    "    partner_factor = np.where(df['partner'] == 'Yes', -0.08, 0)  # Con pareja\n",
    "    dependents_factor = np.where(df['dependents'] == 'Yes', -0.05, 0)  # Con dependientes\n",
    "    long_tenure_factor = np.where(df['tenure'] > 48, -0.12, 0)  # Clientes antiguos\n",
    "    \n",
    "    # Calcular probabilidad final\n",
    "    final_probability = (churn_probability + tenure_factor + contract_factor + \n",
    "                        payment_factor + charges_factor + partner_factor + \n",
    "                        dependents_factor + long_tenure_factor)\n",
    "    \n",
    "    # Generar etiquetas de churn\n",
    "    df['churn'] = np.random.binomial(1, final_probability.clip(0.05, 0.85))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Crear el dataset\n",
    "print(\"Generando dataset sintético de churn...\")\n",
    "churn_data = create_churn_dataset(n_samples=10000)\n",
    "\n",
    "print(f\"Dataset creado:\")\n",
    "print(f\"   • Muestras: {len(churn_data)}\")\n",
    "print(f\"   • Características: {churn_data.shape[1]-2}\")  # -2 para customer_id y churn\n",
    "print(f\"   • Tasa de churn: {churn_data['churn'].mean():.1%}\")\n",
    "\n",
    "# Mostrar primeras filas\n",
    "churn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41212f30-8fd3-46bc-b753-30031c19dc63",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento del Modelo con Pipelines de Scikit-learn\n",
    "\n",
    "### ¿Por qué usar Pipelines?\n",
    "\n",
    "Los **pipelines** de scikit-learn combinan múltiples pasos de preprocesamiento y modelado en un solo objeto, lo que:\n",
    "- Simplifica el código\n",
    "- Evita errores de data leakage\n",
    "- Facilita la serialización\n",
    "- Permite usar el modelo con datos en formato crudo\n",
    "\n",
    "Pensemos en un **Pipeline** de Scikit-learn como una **receta de cocina** o una **línea de ensamblaje** para tu modelo de Machine Learning.\n",
    "\n",
    "En lugar de realizar cada paso por separado (lavar los ingredientes, cortarlos, mezclarlos, hornearlos), un Pipeline te permite definir toda la secuencia de una vez. Le entregas los ingredientes crudos (tus datos) al principio, y al final obtienes el plato terminado (la predicción).\n",
    "\n",
    "### Simplifica el Código\n",
    "\n",
    "Imagina que necesitas rellenar valores faltantes y luego escalar tus datos antes de entrenar un modelo.\n",
    "\n",
    "**Sin un Pipeline**, tu código se vería así, con pasos separados:\n",
    "\n",
    "```python\n",
    "# 1. Rellenar valores faltantes\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test) # ¡Ojo! Solo 'transform' en test\n",
    "\n",
    "# 2. Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed) # De nuevo, solo 'transform'\n",
    "\n",
    "# 3. Entrenar el modelo\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "Es fácil cometer errores, como aplicar `fit_transform` en el conjunto de prueba por accidente.\n",
    "\n",
    "**Con un Pipeline**, todos esos pasos se encapsulan en uno solo:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Definimos la \"receta\" completa\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # Paso 1: Rellenar\n",
    "    ('scaler', StandardScaler()),              # Paso 2: Escalar\n",
    "    ('model', LogisticRegression())            # Paso 3: Modelo\n",
    "])\n",
    "\n",
    "# Entrenamos todo el pipeline de una vez\n",
    "pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "El código es más **limpio, corto y legible**.\n",
    "\n",
    "### Evita Errores de \"Data Leakage\" (Fuga de Datos)\n",
    "\n",
    "La **fuga de datos** es uno de los errores más peligrosos en Machine Learning. Ocurre cuando la información del conjunto de prueba (datos que el modelo \"no debería haber visto\") se \"filtra\" accidentalmente en el proceso de entrenamiento.\n",
    "\n",
    "Piénsalo como si un estudiante **viera las respuestas del examen final mientras estudia**. Obviamente, sacará una nota perfecta en el examen, pero no habrá aprendido nada y no podrá resolver problemas nuevos.\n",
    "\n",
    "Un Pipeline evita esto porque garantiza que cada paso (como el escalado de datos) se **ajuste (`fit`) únicamente con los datos de entrenamiento** y luego solo se **aplique (`transform`)** a los datos de prueba o a nuevos datos, imitando perfectamente las condiciones del mundo real.\n",
    "\n",
    "### Facilita la Serialización (Guardar el Modelo)\n",
    "\n",
    "Cuando quieres guardar tu trabajo, no solo necesitas el modelo, sino también todos los pasos de preprocesamiento que lo acompañan (el `imputer`, el `scaler`, etc.).\n",
    "\n",
    "Sin un Pipeline, tendrías que guardar cada objeto por separado, lo cual es engorroso y propenso a errores. Con un Pipeline, **guardas un solo objeto** que contiene toda la secuencia de trabajo. Es como guardar el archivo de una receta completa en lugar de una lista desordenada de ingredientes y pasos.\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Guardas TODO el flujo de trabajo en un solo archivo\n",
    "joblib.dump(pipeline, 'modelo_completo.pkl')\n",
    "\n",
    "# Para cargarlo, es igual de simple\n",
    "loaded_pipeline = joblib.load('modelo_completo.pkl')\n",
    "```\n",
    "\n",
    "### Permite Usar el Modelo con Datos Crudos\n",
    "\n",
    "Esta es la consecuencia más práctica. Una vez que tu Pipeline está entrenado y guardado, puedes darle **datos nuevos y sin procesar** (datos \"crudos\"), y él se encargará de aplicar automáticamente toda la secuencia de preprocesamiento antes de hacer la predicción.\n",
    "\n",
    "```python\n",
    "# Datos nuevos, tal como llegan del mundo real\n",
    "new_data = [[5.1, 3.5, None, 0.2]] # Tiene un valor faltante\n",
    "\n",
    "# El pipeline se encarga de todo: imputa, escala y predice\n",
    "prediction = loaded_pipeline.predict(new_data)\n",
    "print(prediction)\n",
    "```\n",
    "\n",
    "Esto hace que poner tu modelo en producción sea **infinitamente más sencillo y seguro**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd85a76-6c4a-4ddf-90c0-4f5e06a2b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Preparar características para el modelo.\n",
    "    Convertir DataFrame a lista de diccionarios (formato requerido por DictVectorizer)\n",
    "    \"\"\"\n",
    "    # Seleccionar características relevantes\n",
    "    feature_columns = [\n",
    "        'gender', 'senior_citizen', 'partner', 'dependents', 'tenure',\n",
    "        'phone_service', 'internet_service', 'online_security', 'tech_support',\n",
    "        'contract', 'payment_method', 'monthly_charges', 'total_charges'\n",
    "    ]\n",
    "    \n",
    "    # Convertir a lista de diccionarios\n",
    "    X = df[feature_columns].to_dict('records')\n",
    "    y = df['churn']\n",
    "    \n",
    "    return X, y, feature_columns\n",
    "\n",
    "def train_churn_model(df, model_type='logistic_regression'):\n",
    "    \"\"\"\n",
    "    Entrenar modelo de predicción de churn usando pipelines.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con los datos\n",
    "        model_type: Tipo de modelo ('logistic_regression' o 'random_forest')\n",
    "    \n",
    "    Returns:\n",
    "        pipeline: Modelo entrenado\n",
    "        metrics: Métricas de evaluación\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Iniciando entrenamiento de modelo: {model_type}\")\n",
    "    \n",
    "    # Preparar datos\n",
    "    X, y, feature_columns = prepare_features(df)\n",
    "    \n",
    "    # Split train/validation/test (60/20/20)\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"📊 Split de datos:\")\n",
    "    print(f\"   • Entrenamiento: {len(X_train)} muestras\")\n",
    "    print(f\"   • Validación: {len(X_val)} muestras\")\n",
    "    print(f\"   • Prueba: {len(X_test)} muestras\")\n",
    "    \n",
    "    # Crear pipeline según el tipo de modelo\n",
    "    if model_type == 'logistic_regression':\n",
    "        pipeline = make_pipeline(\n",
    "            DictVectorizer(sparse=False),\n",
    "            LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced'  # Manejar desbalance\n",
    "            )\n",
    "        )\n",
    "    elif model_type == 'random_forest':\n",
    "        pipeline = make_pipeline(\n",
    "            DictVectorizer(sparse=False),\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                max_depth=10\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"model_type debe ser 'logistic_regression' o 'random_forest'\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"🔄 Entrenando modelo...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluación en conjunto de validación\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "    y_val_prob = pipeline.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Evaluación en conjunto de prueba\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    y_test_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Métricas\n",
    "    val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "    \n",
    "    metrics = {\n",
    "        'model_type': model_type,\n",
    "        'validation_auc': val_auc,\n",
    "        'test_auc': test_auc,\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'feature_count': len(feature_columns),\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✅ Entrenamiento completado!\")\n",
    "    print(f\"📈 AUC Validación: {val_auc:.4f}\")\n",
    "    print(f\"📈 AUC Prueba: {test_auc:.4f}\")\n",
    "    \n",
    "    # Reporte detallado\n",
    "    print(f\"\\n📋 Reporte de Clasificación (Conjunto de Prueba):\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=['No Churn', 'Churn']))\n",
    "    \n",
    "    return pipeline, metrics, (X_test, y_test)\n",
    "\n",
    "# Entrenar modelos\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTRENAMIENTO DE MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Modelo 1: Regresión Logística\n",
    "lr_model, lr_metrics, (X_test, y_test) = train_churn_model(\n",
    "    churn_data, \n",
    "    model_type='logistic_regression'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Modelo 2: Random Forest\n",
    "rf_model, rf_metrics, _ = train_churn_model(\n",
    "    churn_data, \n",
    "    model_type='random_forest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c70e3-35a7-4321-b5ef-9cc2ad5188be",
   "metadata": {},
   "source": [
    "### Guardar Modelos Entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c15f21-65e0-42da-bf68-d243dba6cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model_with_metadata(pipeline, metrics, model_name, models_dir=\"./models\"):\n",
    "    \"\"\"\n",
    "    Guardar modelo y sus metadatos de forma organizada.\n",
    "    \"\"\"\n",
    "    # Crear directorio si no existe\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Nombres de archivos\n",
    "    model_filename = f\"{model_name}_{datetime.now().strftime('%Y%m%d')}.joblib\"\n",
    "    metadata_filename = f\"{model_name}_metadata.json\"\n",
    "    \n",
    "    model_path = os.path.join(models_dir, model_filename)\n",
    "    metadata_path = os.path.join(models_dir, metadata_filename)\n",
    "    \n",
    "    # Guardar modelo usando joblib (más eficiente que pickle para sklearn)\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    \n",
    "    # Guardar metadatos\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"💾 Modelo guardado: {model_path}\")\n",
    "    print(f\"📄 Metadatos guardados: {metadata_path}\")\n",
    "    \n",
    "    return model_path, metadata_path\n",
    "\n",
    "# Guardar ambos modelos\n",
    "lr_model_path, lr_metadata_path = save_model_with_metadata(\n",
    "    lr_model, lr_metrics, \"churn_logistic_regression\"\n",
    ")\n",
    "\n",
    "rf_model_path, rf_metadata_path = save_model_with_metadata(\n",
    "    rf_model, rf_metrics, \"churn_random_forest\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Modelos guardados exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c169cc-0164-45b1-9908-bca03eda7c69",
   "metadata": {},
   "source": [
    "## 4. Creación de API con FastAPI\n",
    "\n",
    "Piensa en una API (Interfaz de Programación de Aplicaciones) como un **camarero en un restaurante**. Tú (el cliente) no necesitas saber cómo funciona la cocina; solo le das tu pedido al camarero, él lo lleva a la cocina, y te trae el plato listo. La API hace exactamente eso, pero con datos.\n",
    "\n",
    "FastAPI es un framework que te permite construir a ese \"camarero\" de una manera increíblemente eficiente, rápida y moderna.\n",
    "\n",
    "### Velocidad\n",
    "\n",
    "FastAPI está construido sobre dos pilares de alto rendimiento:\n",
    "\n",
    "1.  **Starlette**: Es un microframework web ultrarrápido. FastAPI lo usa como su motor principal para manejar las peticiones web.\n",
    "2.  **Pydantic**: Se encarga de la validación de datos y está escrito en parte en Rust, lo que lo hace extremadamente veloz.\n",
    "\n",
    "Gracias a esto, FastAPI es uno de los frameworks de Python más rápidos que existen, comparable en rendimiento a aplicaciones escritas en lenguajes compilados como Go o Node.js. Esto significa que tu API puede atender a muchos más usuarios al mismo tiempo sin ralentizarse.\n",
    "\n",
    "### Documentación Automática\n",
    "\n",
    "Este es uno de los superpoderes de FastAPI. Imagina que cada vez que escribes el código de tu API, se **escribe solo un manual de instrucciones interactivo**.\n",
    "\n",
    "FastAPI genera automáticamente una documentación en dos formatos:\n",
    "\n",
    "  * **Swagger UI**\n",
    "  * **ReDoc**\n",
    "\n",
    "Solo tienes que ir a la URL `/docs` de tu API, y encontrarás una página donde puedes ver todos tus *endpoints* (las diferentes \"órdenes\" que tu camarero puede tomar), qué datos necesitan, y qué datos devuelven. ¡Incluso puedes probar la API directamente desde esa página\\! Esto ahorra una cantidad enorme de tiempo en documentación y facilita el trabajo en equipo.\n",
    "\n",
    "### Validación con Pydantic\n",
    "\n",
    "Piensa en Pydantic como el **guardia de seguridad de tu API**. Antes de que cualquier dato entre a tu lógica, Pydantic lo revisa para asegurarse de que tiene el formato correcto.\n",
    "\n",
    "Tú defines la \"forma\" de los datos que esperas usando clases de Python, y Pydantic se encarga del resto.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Item(BaseModel):\n",
    "    name: str\n",
    "    price: float\n",
    "    is_offer: bool | None = None\n",
    "```\n",
    "\n",
    "Si alguien intenta enviar un `price` que no es un número, FastAPI automáticamente le devolverá un error claro y descriptivo. Esto hace tu código mucho más seguro y robusto, evitando errores inesperados.\n",
    "\n",
    "### Soporte Asíncrono (Async)\n",
    "\n",
    "Imagina un chef que solo puede hacer una cosa a la vez (síncrono). Si está esperando que el agua hierva, no puede hacer nada más.\n",
    "\n",
    "Un chef asíncrono, en cambio, pone el agua a hervir y, **mientras espera**, se pone a cortar las verduras. Es mucho más eficiente.\n",
    "\n",
    "FastAPI te permite usar `async` y `await` para manejar operaciones que toman tiempo (como llamar a otra API o consultar una base de datos) sin bloquear todo el programa. Esto es fundamental para construir aplicaciones que necesitan manejar muchas conexiones simultáneas de manera eficiente.\n",
    "\n",
    "### Tipado Nativo de Python (Type Hints)\n",
    "\n",
    "FastAPI utiliza las **pistas de tipos** de Python (`str`, `int`, `bool`, etc.) para todo. No tienes que aprender una sintaxis nueva.\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/items/{item_id}\")\n",
    "def read_item(item_id: int, q: str | None = None):\n",
    "    return {\"item_id\": item_id, \"q\": q}\n",
    "```\n",
    "\n",
    "Al declarar que `item_id` debe ser un `int`, FastAPI automáticamente:\n",
    "\n",
    "1.  **Valida** que el dato recibido es un entero.\n",
    "2.  **Documenta** que este endpoint espera un entero.\n",
    "3.  Le da a tu editor de código (como VS Code) información para **autocompletar** y detectar errores mientras escribes.\n",
    "\n",
    "En resumen, FastAPI usa características modernas de Python para darte una experiencia de desarrollo rápida, segura y muy agradable.\n",
    "\n",
    "### Decoradores\n",
    "\n",
    "Un **decorador** en Python es como ponerle un \"sombrero\" especial a una función para darle superpoderes o un nuevo comportamiento. Usas el símbolo `@` para aplicarlo.\n",
    "\n",
    "En FastAPI, los decoradores le dicen a tu \"camarero\" (la API) qué hacer cuando alguien llega a una URL específica con un método de petición concreto (GET, POST, etc.).\n",
    "\n",
    "### Los \"Sombreros\" de Operación: GET, POST, PUT, DELETE\n",
    "\n",
    "Piensa en estos decoradores como las diferentes tareas que un camarero puede realizar: tomar una orden, entregar un plato, actualizar una orden o cancelarla.\n",
    "\n",
    "  * **`@app.get(\"/ruta\")`**: **Leer datos.** Se usa cuando un cliente quiere *obtener* información. Es como preguntar el menú del día. Es la operación más común.\n",
    "\n",
    "    ```python\n",
    "    @app.get(\"/items/{item_id}\")\n",
    "    def leer_item(item_id: int):\n",
    "        # Aquí iría el código para buscar el item en una base de datos\n",
    "        return {\"item_id\": item_id, \"nombre\": \"Ejemplo de item\"}\n",
    "    ```\n",
    "\n",
    "  * **`@app.post(\"/ruta\")`**: **Crear datos.** Se usa cuando un cliente quiere *añadir* nueva información al sistema. Es como hacer un pedido nuevo en la cocina.\n",
    "\n",
    "    ```python\n",
    "    from pydantic import BaseModel\n",
    "\n",
    "    class Item(BaseModel):\n",
    "        name: str\n",
    "        price: float\n",
    "\n",
    "    @app.post(\"/items/\")\n",
    "    def crear_item(item: Item):\n",
    "        # Aquí guardarías el nuevo 'item' en la base de datos\n",
    "        return {\"mensaje\": f\"Item '{item.name}' creado exitosamente.\"}\n",
    "    ```\n",
    "\n",
    "  * **`@app.put(\"/ruta\")`**: **Actualizar datos.** Se usa para reemplazar o actualizar por completo un recurso existente. Es como cambiar tu pedido por completo.\n",
    "\n",
    "    ```python\n",
    "    @app.put(\"/items/{item_id}\")\n",
    "    def actualizar_item(item_id: int, item: Item):\n",
    "        # Lógica para actualizar el item con el id correspondiente\n",
    "        return {\"item_id\": item_id, **item.dict()}\n",
    "    ```\n",
    "\n",
    "  * **`@app.delete(\"/ruta\")`**: **Borrar datos.** Se usa para eliminar un recurso. Es como cancelar un plato de tu orden.\n",
    "\n",
    "    ```python\n",
    "    @app.delete(\"/items/{item_id}\")\n",
    "    def borrar_item(item_id: int):\n",
    "        # Lógica para borrar el item de la base de datos\n",
    "        return {\"mensaje\": f\"Item con id {item_id} ha sido eliminado.\"}\n",
    "    ```\n",
    "\n",
    "\n",
    "### Parámetros de Ruta y Consultas\n",
    "\n",
    "FastAPI es inteligente y usa los argumentos de tu función para entender los datos que llegan:\n",
    "\n",
    "1.  **Parámetros de Ruta (Path Parameters)**: Son valores que forman parte de la propia URL y se definen con llaves `{}`. En `@app.get(\"/items/{item_id}\")`, `item_id` es un parámetro de ruta. FastAPI entiende que debe extraer ese valor de la URL y pasarlo a tu función.\n",
    "\n",
    "2.  **Parámetros de Consulta (Query Parameters)**: Son parámetros opcionales que van al final de la URL después de un `?`. Por ejemplo, en `/users?role=admin`, `role` es un parámetro de consulta. Si un argumento de tu función **no** está en la ruta, FastAPI asume que es un parámetro de consulta.\n",
    "\n",
    "    ```python\n",
    "    @app.get(\"/users/\")\n",
    "    # 'limit' es un parámetro de consulta con un valor por defecto.\n",
    "    # Se usaría así: /users/ o /users/?limit=50\n",
    "    def leer_usuarios(limit: int = 100):\n",
    "        # ...lógica para devolver usuarios...\n",
    "        return {\"limite\": limit, \"usuarios\": []}\n",
    "    ```\n",
    "\n",
    "### El Cuerpo de la Petición (Request Body)\n",
    "\n",
    "Para operaciones como `POST` y `PUT`, los datos suelen ser demasiado complejos para ir en la URL. En su lugar, se envían en el \"cuerpo\" de la petición, normalmente como un objeto JSON.\n",
    "\n",
    "Aquí es donde usas un **modelo de Pydantic**. Al declarar un argumento de tu función con el tipo de un modelo Pydantic (como `item: Item`), le dices a FastAPI:\n",
    "\n",
    "1.  Espera recibir un JSON en el cuerpo de la petición.\n",
    "2.  Verifica que el JSON tenga la misma estructura que la clase `Item`.\n",
    "3.  Si es válido, convierte el JSON en un objeto de Python y pásalo a mi función.\n",
    "4.  Si no es válido, responde automáticamente con un error claro.\n",
    "\n",
    "Esto hace que manejar datos de entrada complejos sea increíblemente simple y seguro.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8207ff63-ff36-42d1-9841-4578206c690f",
   "metadata": {},
   "source": [
    "### Aplicación FastAPI Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0bcea-4fbb-4ec7-8038-a9df229caec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/main.py\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from contextlib import asynccontextmanager\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "from schemas.predictions import CustomerInput, PredictionResponse, BatchPredictionRequest\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Almacenamiento global para modelos\n",
    "ml_models: Dict[str, Any] = {}\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Gestión del ciclo de vida de la aplicación\"\"\"\n",
    "    logger.info(\"🚀 Iniciando carga de modelos...\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar modelos disponibles\n",
    "        models_dir = \"models\"\n",
    "        if os.path.exists(models_dir):\n",
    "            for filename in os.listdir(models_dir):\n",
    "                if filename.endswith('.joblib'):\n",
    "                    model_name = filename.replace('.joblib', '')\n",
    "                    model_path = os.path.join(models_dir, filename)\n",
    "                    \n",
    "                    model = joblib.load(model_path)\n",
    "                    ml_models[model_name] = model\n",
    "                    logger.info(f\"✅ Modelo cargado: {model_name}\")\n",
    "        \n",
    "        if not ml_models:\n",
    "            logger.warning(\"⚠️ No se encontraron modelos en el directorio\")\n",
    "        \n",
    "        logger.info(f\"📊 Total modelos cargados: {len(ml_models)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error cargando modelos: {e}\")\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    # Limpieza al cerrar\n",
    "    ml_models.clear()\n",
    "    logger.info(\"🔄 Recursos liberados\")\n",
    "\n",
    "# Crear aplicación FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Churn Prediction API\",\n",
    "    description=\"\"\"\n",
    "    🎯 **API para Predicción de Churn de Clientes**\n",
    "    \n",
    "    Esta API utiliza modelos de Machine Learning para predecir la probabilidad\n",
    "    de que un cliente cancele su servicio (churn).\n",
    "    \n",
    "    ## Características\n",
    "    \n",
    "    * **Predicciones individuales**: Predice churn para un cliente\n",
    "    * **Predicciones por lotes**: Procesa múltiples clientes\n",
    "    * **Múltiples modelos**: Soporte para diferentes algoritmos\n",
    "    * **Validación automática**: Verificación de datos de entrada\n",
    "    * **Documentación interactiva**: Swagger UI integrado\n",
    "    \n",
    "    ## Modelos Disponibles\n",
    "    \n",
    "    * **Regresión Logística**: Modelo interpretable y rápido\n",
    "    * **Random Forest**: Modelo ensemble con alta precisión\n",
    "    \"\"\",\n",
    "    version=\"1.0.0\",\n",
    "    contact={\n",
    "        \"name\": \"Equipo ML\",\n",
    "        \"email\": \"ml@tuempresa.com\",\n",
    "    },\n",
    "    license_info={\n",
    "        \"name\": \"Apache 2.0\",\n",
    "        \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n",
    "    },\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "# Configurar CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # En producción, especifica dominios específicos\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Dependencia para obtener modelo\n",
    "async def get_model(model_name: str = \"churn_logistic_regression_20241201\"):\n",
    "    if model_name not in ml_models:\n",
    "        available_models = list(ml_models.keys())\n",
    "        raise HTTPException(\n",
    "            status_code=404,\n",
    "            detail=f\"Modelo '{model_name}' no encontrado. Modelos disponibles: {available_models}\"\n",
    "        )\n",
    "    return ml_models[model_name]\n",
    "\n",
    "# === ENDPOINTS ===\n",
    "\n",
    "@app.get(\"/\", tags=[\"info\"])\n",
    "async def root():\n",
    "    \"\"\"Información básica de la API\"\"\"\n",
    "    return {\n",
    "        \"message\": \"🎯 Churn Prediction API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"status\": \"active\",\n",
    "        \"models_loaded\": len(ml_models),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\", tags=[\"monitoring\"])\n",
    "async def health_check():\n",
    "    \"\"\"Health check para monitoreo\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"models_count\": len(ml_models),\n",
    "        \"models_available\": list(ml_models.keys()),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "@app.get(\"/models\", tags=[\"models\"])\n",
    "async def list_models():\n",
    "    \"\"\"Listar modelos disponibles\"\"\"\n",
    "    model_info = {}\n",
    "    \n",
    "    for name, model in ml_models.items():\n",
    "        model_info[name] = {\n",
    "            \"type\": type(model).__name__,\n",
    "            \"steps\": [step[0] for step in model.steps] if hasattr(model, 'steps') else \"Pipeline\",\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"available_models\": model_info,\n",
    "        \"total_count\": len(ml_models)\n",
    "    }\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse, tags=[\"predictions\"])\n",
    "async def predict_churn(\n",
    "    customer: CustomerInput,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    model_name: str = \"churn_logistic_regression_20241201\",\n",
    "    model = Depends(get_model)\n",
    "):\n",
    "    \"\"\"\n",
    "    🎯 Predecir probabilidad de churn para un cliente\n",
    "    \n",
    "    Utiliza el modelo especificado para calcular la probabilidad de que\n",
    "    el cliente cancele su servicio.\n",
    "    \n",
    "    - **customer**: Datos del cliente (ver esquema completo abajo)\n",
    "    - **model_name**: Nombre del modelo a utilizar\n",
    "    \n",
    "    Retorna predicción, probabilidades y metadatos del modelo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Convertir datos de entrada a formato de diccionario\n",
    "        customer_dict = customer.model_dump()\n",
    "        \n",
    "        # Hacer predicción\n",
    "        prediction = model.predict([customer_dict])[0]\n",
    "        probabilities = model.predict_proba([customer_dict])[0]\n",
    "        \n",
    "        # Calcular métricas\n",
    "        churn_probability = float(probabilities[1])\n",
    "        retention_probability = float(probabilities[0])\n",
    "        confidence = max(probabilities)\n",
    "        \n",
    "        # Determinar categoría de riesgo\n",
    "        if churn_probability >= 0.7:\n",
    "            risk_category = \"High\"\n",
    "        elif churn_probability >= 0.4:\n",
    "            risk_category = \"Medium\"  \n",
    "        else:\n",
    "            risk_category = \"Low\"\n",
    "        \n",
    "        processing_time = (datetime.now() - start_time).total_seconds() * 1000\n",
    "        \n",
    "        response = PredictionResponse(\n",
    "            prediction=int(prediction),\n",
    "            churn_probability=churn_probability,\n",
    "            retention_probability=retention_probability,\n",
    "            risk_category=risk_category,\n",
    "            confidence=float(confidence),\n",
    "            model_info={\n",
    "                \"name\": model_name,\n",
    "                \"type\": type(model).__name__,\n",
    "                \"version\": \"1.0.0\"\n",
    "            },\n",
    "            processing_time_ms=int(processing_time),\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "        \n",
    "        # Log en background (no bloquea la respuesta)\n",
    "        background_tasks.add_task(\n",
    "            log_prediction,\n",
    "            customer_dict,\n",
    "            response.model_dump(),\n",
    "            model_name\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en predicción: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error procesando predicción: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# Función de logging asíncrono\n",
    "async def log_prediction(customer_data: dict, prediction_result: dict, model_name: str):\n",
    "    \"\"\"Registrar predicción para monitoreo y análisis\"\"\"\n",
    "    logger.info(\n",
    "        f\"PREDICTION - Model: {model_name}, \"\n",
    "        f\"Churn_Prob: {prediction_result['churn_probability']:.3f}, \"\n",
    "        f\"Risk: {prediction_result['risk_category']}, \"\n",
    "        f\"Tenure: {customer_data.get('tenure', 'N/A')}\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(\n",
    "        \"main:app\",\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        reload=True,\n",
    "        log_level=\"info\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bf7a0-1171-45b6-b954-844ea3f7b5f0",
   "metadata": {},
   "source": [
    "## 5. Modelos de Validación con Pydantic\n",
    "\n",
    "Imagina que tu aplicación es un club exclusivo y los datos que intentan entrar son los invitados. Pydantic es el **guardia de seguridad en la puerta** . Tú le das al guardia una lista estricta de invitados (`BaseModel`), y él se encarga de:\n",
    "\n",
    "1.  **Verificar la identidad**: Comprueba que los datos que llegan tienen los campos que esperas (nombre, edad, etc.).\n",
    "2.  **Revisar la edad**: Se asegura de que cada dato sea del tipo correcto (que la edad sea un número, no texto).\n",
    "3.  **No dejar entrar a cualquiera**: Rechaza los datos que no cumplen las reglas y te dice exactamente por qué.\n",
    "\n",
    "En resumen, Pydantic usa las pistas de tipos de Python (`type hints`) para definir la \"forma\" que deben tener tus datos y luego se asegura de que se cumpla.\n",
    "\n",
    "### ¿Cómo se usa?\n",
    "\n",
    "Para definir tu \"lista de invitados\", creas una clase que hereda de `BaseModel`.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class Usuario(BaseModel):\n",
    "    id: int\n",
    "    nombre: str\n",
    "    es_miembro_activo: bool\n",
    "    intereses: List[str] = []  # Valor por defecto: una lista vacía\n",
    "    edad: Optional[int] = None # Campo opcional (puede ser int o None)\n",
    "```\n",
    "\n",
    "Este modelo le dice a Pydantic: \"Un `Usuario` **debe** tener un `id` (entero), un `nombre` (texto) y un estado `es_miembro_activo` (booleano). Opcionalmente, puede tener una `edad` (entero) y una lista de `intereses`.\"\n",
    "\n",
    "### La Validación en Acción\n",
    "\n",
    "Ahora, cuando recibes datos (por ejemplo, un JSON de una API), se los pasas al modelo.\n",
    "\n",
    "```python\n",
    "datos_externos = {\n",
    "    \"id\": \"123\", # ¡Ojo, es un string!\n",
    "    \"nombre\": \"Gema\",\n",
    "    \"es_miembro_activo\": \"true\", # ¡Otro string!\n",
    "    \"intereses\": [\"AI\", \"Python\"]\n",
    "}\n",
    "\n",
    "try:\n",
    "    usuario_validado = Usuario(**datos_externos)\n",
    "    print(usuario_validado)\n",
    "    # SALIDA:\n",
    "    # id=123 es_miembro_activo=True intereses=['AI', 'Python'] edad=None nombre='Gema'\n",
    "    \n",
    "    print(usuario_validado.id)\n",
    "    # SALIDA:\n",
    "    # 123 (¡como un entero!)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "```\n",
    "\n",
    "Pydantic no solo **valida**, sino que también **convierte** los datos al tipo correcto (el `\"123\"` se convierte en el número `123`). Si los datos fueran incorrectos (por ejemplo, `id: \"hola\"`), Pydantic generaría un error muy claro que te diría exactamente qué campo está mal y por qué.\n",
    "\n",
    "Esto hace que tu código sea extremadamente **robusto y seguro**, especialmente cuando trabajas con APIs como FastAPI, donde Pydantic es una pieza fundamental.\n",
    "\n",
    "\n",
    "### Exportando Modelos\n",
    "\n",
    "Una vez que Pydantic ha validado y creado tu objeto, a menudo necesitas convertirlo de nuevo a un formato estándar, como un diccionario de Python o un string JSON (por ejemplo, para enviarlo como respuesta en una API). Pydantic hace esto trivial.\n",
    "\n",
    "  - **`.dict()`**: Convierte el modelo a un diccionario de Python.\n",
    "  - **`.json()`**: Convierte el modelo directamente a un string con formato JSON.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Producto(BaseModel):\n",
    "    id: int\n",
    "    nombre: str\n",
    "    stock: int = 0\n",
    "\n",
    "# Creamos una instancia del modelo\n",
    "producto_a = Producto(id=1, nombre=\"Laptop Pro\", stock=55)\n",
    "\n",
    "# Lo convertimos a un diccionario\n",
    "print(producto_a.dict())\n",
    "# SALIDA: {'id': 1, 'nombre': 'Laptop Pro', 'stock': 55}\n",
    "\n",
    "# Lo convertimos a un string JSON\n",
    "print(producto_a.json())\n",
    "# SALIDA: '{\"id\": 1, \"nombre\": \"Laptop Pro\", \"stock\": 55}'\n",
    "```\n",
    "\n",
    "### Personalización de Campos con `Field`\n",
    "\n",
    "A veces, no basta con definir un tipo. Es posible que necesites añadir más reglas o metadatos a un campo, como validaciones numéricas, longitudes máximas, o un nombre diferente para cuando los datos vienen de fuera. Para esto se usa `Field`.\n",
    "\n",
    "Una de sus funciones más útiles es el **alias**. Imagina que una API externa te envía datos en `camelCase` (`productId`), pero en Python prefieres usar `snake_case` (`product_id`). Pydantic maneja esta conversión por ti.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Artista(BaseModel):\n",
    "    # Pydantic esperará una clave \"artistName\" en los datos de entrada\n",
    "    nombre_artista: str = Field(alias='artistName')\n",
    "    edad: int = Field(\n",
    "        gt=0, # gt = greater than (mayor que 0)\n",
    "        le=120, # le = less than or equal (menor o igual a 120)\n",
    "        description=\"La edad del artista debe estar entre 1 y 120.\"\n",
    "    )\n",
    "\n",
    "datos_externos = {\"artistName\": \"Leo\", \"edad\": 35}\n",
    "artista_validado = Artista(**datos_externos)\n",
    "\n",
    "print(artista_validado.nombre_artista) # Imprime \"Leo\"\n",
    "```\n",
    "\n",
    "Con `Field` puedes añadir restricciones como: **`gt`** (mayor que), **`lt`** (menor que), **`max_length`**, **`min_length`**, etc.\n",
    "\n",
    "### Validadores Personalizados (`@validator`)\n",
    "\n",
    "Mientras que `Field` te da reglas predefinidas (como `gt` o `max_length`), `@validator` te permite crear **tus propias funciones de validación** para implementar cualquier lógica que necesites. Es como pasar de una lista de reglas a contratar a un detective que puede hacer una investigación a fondo.\n",
    "\n",
    "Se aplica como un decorador a un método dentro de tu clase. Este método recibe el valor del campo y debe devolverlo (posiblemente transformado) o lanzar un `ValueError` si no es válido.\n",
    "\n",
    "**Ejemplo**: Asegurarse de que un nombre de usuario no contenga caracteres especiales.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class Usuario(BaseModel):\n",
    "    username: str\n",
    "\n",
    "    @validator('username')\n",
    "    def username_no_debe_contener_especiales(cls, v):\n",
    "        # v es el valor del campo 'username'\n",
    "        if not v.isalnum(): # isalnum() comprueba si es alfanumérico\n",
    "            raise ValueError('El nombre de usuario solo puede contener letras y números.')\n",
    "        return v\n",
    "\n",
    "# Esto funcionará\n",
    "usuario_ok = Usuario(username='gema123')\n",
    "\n",
    "# Esto lanzará el ValueError que definimos\n",
    "try:\n",
    "    usuario_malo = Usuario(username='gema-123!')\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "```\n",
    "\n",
    "### Validadores Raíz (`@root_validator`)\n",
    "\n",
    "Un validador raíz es aún más potente: se ejecuta **después de todos los validadores de campos individuales** y tiene acceso a **todos los datos del modelo** a la vez. Es perfecto para validaciones que dependen de la relación entre varios campos.\n",
    "\n",
    "**Ejemplo**: Confirmar que dos campos de contraseña coinciden.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, root_validator\n",
    "\n",
    "class FormularioRegistro(BaseModel):\n",
    "    password: str\n",
    "    confirm_password: str\n",
    "\n",
    "    @root_validator()\n",
    "    def las_contraseñas_deben_coincidir(cls, values):\n",
    "        # 'values' es un diccionario con todos los campos: {'password': '...', 'confirm_password': '...'}\n",
    "        password = values.get('password')\n",
    "        confirm_password = values.get('confirm_password')\n",
    "\n",
    "        if password is not None and password != confirm_password:\n",
    "            raise ValueError('Las contraseñas no coinciden.')\n",
    "        \n",
    "        return values\n",
    "\n",
    "# Esto funcionará\n",
    "formulario_ok = FormularioRegistro(password='1234', confirm_password='1234')\n",
    "\n",
    "# Esto lanzará el ValueError\n",
    "try:\n",
    "    formulario_malo = FormularioRegistro(password='1234', confirm_password='abcd')\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "```\n",
    "\n",
    "### Configuración del Modelo (clase `Config`)\n",
    "\n",
    "Puedes cambiar el comportamiento general de tu modelo Pydantic añadiendo una clase interna llamada `Config`. Esto te permite ajustar muchas opciones.\n",
    "\n",
    "Algunas de las más útiles son:\n",
    "\n",
    "  - **`orm_mode = True`**: Permite que el modelo se cree a partir de objetos de un ORM (como SQLAlchemy), leyendo sus atributos directamente. Esencial para trabajar con bases de datos.\n",
    "  - **`anystr_strip_whitespace = True`**: Elimina automáticamente los espacios en blanco al principio y al final de todos los campos de texto.\n",
    "  - **`validate_assignment = True`**: Hace que el modelo vuelva a validar un campo cada vez que le asignas un nuevo valor, no solo durante la creación.\n",
    "  - **`allow_population_by_field_name = True`**: Permite poblar el modelo usando tanto el nombre del campo como su alias.\n",
    "\n",
    "**Ejemplo**:\n",
    "\n",
    "```python\n",
    "class Usuario(BaseModel):\n",
    "    nombre: str\n",
    "\n",
    "    class Config:\n",
    "        anystr_strip_whitespace = True\n",
    "\n",
    "# Pydantic eliminará los espacios automáticamente\n",
    "usuario = Usuario(nombre=\"  Usera  \")\n",
    "print(usuario.nombre) # Salida: \"Usera\"\n",
    "```\n",
    "\n",
    "En resumen, con los validadores y la clase `Config`, tienes un control total sobre cómo tus modelos interpretan, validan y gestionan los datos.\n",
    "\n",
    "### Modelos Anidados\n",
    "\n",
    "Los datos del mundo real rara vez son planos. Es muy común tener objetos dentro de otros objetos. Pydantic maneja esto de forma muy natural: simplemente usas un modelo como el tipo de otro campo.\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "class Cancion(BaseModel):\n",
    "    titulo: str\n",
    "    duracion_seg: int\n",
    "\n",
    "class Album(BaseModel):\n",
    "    titulo_album: str\n",
    "    año_lanzamiento: int\n",
    "    canciones: List[Cancion] # ¡Una lista de otros modelos!\n",
    "\n",
    "datos_album = {\n",
    "    \"titulo_album\": \"Grandes Éxitos\",\n",
    "    \"año_lanzamiento\": 2024,\n",
    "    \"canciones\": [\n",
    "        {\"titulo\": \"Mi Primera Canción\", \"duracion_seg\": 180},\n",
    "        {\"titulo\": \"El Hit del Verano\", \"duracion_seg\": 210}\n",
    "    ]\n",
    "}\n",
    "\n",
    "album_obj = Album(**datos_album)\n",
    "\n",
    "# Accedes a los datos de forma intuitiva\n",
    "print(album_obj.titulo_album) # Imprime \"Grandes Éxitos\"\n",
    "print(album_obj.canciones[0].titulo) # Imprime \"Mi Primera Canción\"\n",
    "```\n",
    "\n",
    "### Tipos Comunes Pre-validados\n",
    "\n",
    "Pydantic viene con una gran variedad de **tipos de datos ya preparados** que tienen validaciones complejas incorporadas, ahorrándote el trabajo de escribirlas tú mismo.\n",
    "\n",
    "Estos tipos especiales se importan directamente de Pydantic o de la biblioteca estándar de Python y se usan como cualquier otro tipo (`str`, `int`, etc.).\n",
    "\n",
    "Aquí tienes una lista de algunos de los más útiles con ejemplos.\n",
    "\n",
    "#### 1\\. Direcciones de Correo Electrónico\n",
    "\n",
    "Para validar que una cadena de texto tiene el formato de un email, usas `EmailStr`.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, EmailStr\n",
    "\n",
    "class Usuario(BaseModel):\n",
    "    email: EmailStr\n",
    "\n",
    "# Esto funciona\n",
    "usuario_ok = Usuario(email=\"usuario@google.com\")\n",
    "\n",
    "# Esto fallará con un error de validación\n",
    "try:\n",
    "    usuario_malo = Usuario(email=\"texto-invalido\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "```\n",
    "\n",
    "#### 2\\. URLs\n",
    "\n",
    "Pydantic ofrece varios tipos para validar URLs, siendo `AnyHttpUrl` uno de los más comunes para validar direcciones web `http` o `httpsapps`.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, AnyHttpUrl\n",
    "\n",
    "class PaginaWeb(BaseModel):\n",
    "    url: AnyHttpUrl\n",
    "\n",
    "# Esto funciona\n",
    "pagina_ok = PaginaWeb(url=\"https://www.google.com\")\n",
    "\n",
    "# Esto fallará\n",
    "try:\n",
    "    pagina_mala = PaginaWeb(url=\"ftp://servidor.com\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "```\n",
    "\n",
    "#### 3\\. UUIDs\n",
    "\n",
    "Si trabajas con identificadores únicos universales (UUID), puedes usar el tipo `UUID` de la biblioteca estándar de Python. Pydantic lo soporta nativamente.\n",
    "\n",
    "```python\n",
    "from uuid import UUID\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Pedido(BaseModel):\n",
    "    id_pedido: UUID\n",
    "\n",
    "# Esto funciona\n",
    "pedido_ok = Pedido(id_pedido=\"123e4567-e89b-12d3-a456-426614174000\")\n",
    "\n",
    "# Esto fallará\n",
    "try:\n",
    "    pedido_malo = Pedido(id_pedido=\"no-es-un-uuid\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "```\n",
    "\n",
    "#### 4\\. Tipos Estrictos\n",
    "\n",
    "A veces, quieres evitar que Pydantic convierta tipos (por ejemplo, que `\"123\"` no se convierta en `123`). Para eso, existen los tipos estrictos como `StrictStr` o `StrictInt`.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, StrictInt\n",
    "\n",
    "class Producto(BaseModel):\n",
    "    stock: StrictInt\n",
    "\n",
    "# Esto funciona\n",
    "prod_ok = Producto(stock=50)\n",
    "\n",
    "# Esto fallará porque \"50\" es un string, no un entero\n",
    "try:\n",
    "    prod_malo = Producto(stock=\"50\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "```\n",
    "\n",
    "Esta es solo una pequeña muestra. La documentación de Pydantic tiene una lista completa que incluye tipos para redes (`IPv4Address`), archivos (`FilePath`), colores (`Color`), y muchos más."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227d033-79b6-4bf5-9132-7fa77c94c62c",
   "metadata": {},
   "source": [
    "### Esquemas de Entrada y Salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418dfb20-d02e-46fd-bdd9-8b51bd45d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/schemas/predictions.py\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "class RiskCategory(str, Enum):\n",
    "    \"\"\"Categorías de riesgo de churn\"\"\"\n",
    "    LOW = \"Low\"\n",
    "    MEDIUM = \"Medium\" \n",
    "    HIGH = \"High\"\n",
    "\n",
    "class ContractType(str, Enum):\n",
    "    \"\"\"Tipos de contrato disponibles\"\"\"\n",
    "    MONTH_TO_MONTH = \"Month-to-month\"\n",
    "    ONE_YEAR = \"One year\"\n",
    "    TWO_YEAR = \"Two year\"\n",
    "\n",
    "class PaymentMethod(str, Enum):\n",
    "    \"\"\"Métodos de pago disponibles\"\"\"\n",
    "    ELECTRONIC_CHECK = \"Electronic check\"\n",
    "    MAILED_CHECK = \"Mailed check\"\n",
    "    BANK_TRANSFER = \"Bank transfer (automatic)\"\n",
    "    CREDIT_CARD = \"Credit card (automatic)\"\n",
    "\n",
    "class InternetService(str, Enum):\n",
    "    \"\"\"Tipos de servicio de internet\"\"\"\n",
    "    DSL = \"DSL\"\n",
    "    FIBER_OPTIC = \"Fiber optic\"\n",
    "    NO = \"No\"\n",
    "\n",
    "class CustomerInput(BaseModel):\n",
    "    \"\"\"\n",
    "    Modelo de entrada para datos del cliente.\n",
    "    \n",
    "    Todos los campos son validados automáticamente por Pydantic.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Información demográfica\n",
    "    gender: str = Field(\n",
    "        ..., \n",
    "        description=\"Género del cliente\",\n",
    "        example=\"Male\"\n",
    "    )\n",
    "    \n",
    "    senior_citizen: int = Field(\n",
    "        ..., \n",
    "        ge=0, \n",
    "        le=1,\n",
    "        description=\"Es ciudadano senior (0=No, 1=Sí)\",\n",
    "        example=0\n",
    "    )\n",
    "    \n",
    "    partner: str = Field(\n",
    "        ...,\n",
    "        description=\"Tiene pareja\",\n",
    "        example=\"Yes\"\n",
    "    )\n",
    "    \n",
    "    dependents: str = Field(\n",
    "        ...,\n",
    "        description=\"Tiene dependientes\",\n",
    "        example=\"No\"\n",
    "    )\n",
    "    \n",
    "    # Información del servicio\n",
    "    tenure: int = Field(\n",
    "        ...,\n",
    "        ge=0,\n",
    "        le=100,\n",
    "        description=\"Meses como cliente\",\n",
    "        example=24\n",
    "    )\n",
    "    \n",
    "    phone_service: str = Field(\n",
    "        ...,\n",
    "        description=\"Tiene servicio telefónico\",\n",
    "        example=\"Yes\"\n",
    "    )\n",
    "    \n",
    "    internet_service: InternetService = Field(\n",
    "        ...,\n",
    "        description=\"Tipo de servicio de internet\",\n",
    "        example=\"Fiber optic\"\n",
    "    )\n",
    "    \n",
    "    online_security: str = Field(\n",
    "        ...,\n",
    "        description=\"Tiene seguridad online\",\n",
    "        example=\"No\"\n",
    "    )\n",
    "    \n",
    "    tech_support: str = Field(\n",
    "        ...,\n",
    "        description=\"Tiene soporte técnico\",\n",
    "        example=\"Yes\"\n",
    "    )\n",
    "    \n",
    "    # Información contractual y financiera\n",
    "    contract: ContractType = Field(\n",
    "        ...,\n",
    "        description=\"Tipo de contrato\",\n",
    "        example=\"Month-to-month\"\n",
    "    )\n",
    "    \n",
    "    payment_method: PaymentMethod = Field(\n",
    "        ...,\n",
    "        description=\"Método de pago\",\n",
    "        example=\"Electronic check\"\n",
    "    )\n",
    "    \n",
    "    monthly_charges: float = Field(\n",
    "        ...,\n",
    "        gt=0,\n",
    "        lt=200,\n",
    "        description=\"Cargos mensuales en USD\",\n",
    "        example=85.50\n",
    "    )\n",
    "    \n",
    "    total_charges: float = Field(\n",
    "        ...,\n",
    "        ge=0,\n",
    "        description=\"Total de cargos acumulados en USD\",\n",
    "        example=2052.00\n",
    "    )\n",
    "    \n",
    "    # Campo opcional para ID del cliente\n",
    "    customer_id: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"ID opcional del cliente\",\n",
    "        example=\"CUST001\"\n",
    "    )\n",
    "    \n",
    "    @validator('gender')\n",
    "    def validate_gender(cls, v):\n",
    "        allowed_genders = ['Male', 'Female']\n",
    "        if v not in allowed_genders:\n",
    "            raise ValueError(f'Gender debe ser uno de: {allowed_genders}')\n",
    "        return v\n",
    "    \n",
    "    @validator('partner', 'dependents', 'phone_service')\n",
    "    def validate_yes_no_fields(cls, v, field):\n",
    "        allowed_values = ['Yes', 'No']\n",
    "        if v not in allowed_values:\n",
    "            raise ValueError(f'{field.name} debe ser \"Yes\" o \"No\"')\n",
    "        return v\n",
    "\n",
    "    class Config:\n",
    "        # Configuración del modelo Pydantic\n",
    "        str_strip_whitespace = True  # Eliminar espacios en blanco\n",
    "        validate_assignment = True   # Validar en asignaciones\n",
    "        use_enum_values = True      # Usar valores de enum\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"gender\": \"Female\",\n",
    "                \"senior_citizen\": 0,\n",
    "                \"partner\": \"Yes\",\n",
    "                \"dependents\": \"No\", \n",
    "                \"tenure\": 24,\n",
    "                \"phone_service\": \"Yes\",\n",
    "                \"internet_service\": \"Fiber optic\",\n",
    "                \"online_security\": \"No\",\n",
    "                \"tech_support\": \"Yes\",\n",
    "                \"contract\": \"Month-to-month\",\n",
    "                \"payment_method\": \"Electronic check\",\n",
    "                \"monthly_charges\": 85.50,\n",
    "                \"total_charges\": 2052.00,\n",
    "                \"customer_id\": \"CUST001\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Respuesta de la predicción de churn\"\"\"\n",
    "    \n",
    "    prediction: int = Field(\n",
    "        ...,\n",
    "        description=\"Predicción de churn (0=No Churn, 1=Churn)\",\n",
    "        example=1\n",
    "    )\n",
    "    \n",
    "    churn_probability: float = Field(\n",
    "        ...,\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "        description=\"Probabilidad de churn\",\n",
    "        example=0.75\n",
    "    )\n",
    "    \n",
    "    retention_probability: float = Field(\n",
    "        ...,\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "        description=\"Probabilidad de retención\",\n",
    "        example=0.25\n",
    "    )\n",
    "    \n",
    "    risk_category: RiskCategory = Field(\n",
    "        ...,\n",
    "        description=\"Categoría de riesgo\",\n",
    "        example=\"High\"\n",
    "    )\n",
    "    \n",
    "    confidence: float = Field(\n",
    "        ...,\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "        description=\"Confianza del modelo\",\n",
    "        example=0.75\n",
    "    )\n",
    "    \n",
    "    model_info: Dict[str, Any] = Field(\n",
    "        ...,\n",
    "        description=\"Información del modelo utilizado\",\n",
    "        example={\n",
    "            \"name\": \"churn_logistic_regression\",\n",
    "            \"type\": \"Pipeline\",\n",
    "            \"version\": \"1.0.0\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    processing_time_ms: int = Field(\n",
    "        ...,\n",
    "        ge=0,\n",
    "        description=\"Tiempo de procesamiento en milisegundos\",\n",
    "        example=150\n",
    "    )\n",
    "    \n",
    "    timestamp: datetime = Field(\n",
    "        default_factory=datetime.now,\n",
    "        description=\"Timestamp de la predicción\",\n",
    "        example=\"2024-12-01T10:30:00\"\n",
    "    )\n",
    "\n",
    "class BatchPredictionRequest(BaseModel):\n",
    "    \"\"\"Solicitud de predicción por lotes\"\"\"\n",
    "    \n",
    "    customers: List[CustomerInput] = Field(\n",
    "        ...,\n",
    "        min_items=1,\n",
    "        max_items=1000,  # Límite para evitar sobrecarga\n",
    "        description=\"Lista de clientes para predecir\"\n",
    "    )\n",
    "    \n",
    "    include_details: bool = Field(\n",
    "        True,\n",
    "        description=\"Incluir detalles completos en la respuesta\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984d960-a61a-467d-bbaf-a50967d46df5",
   "metadata": {},
   "source": [
    "## 6. Contenedorización con Docker\n",
    "\n",
    "Docker es una plataforma que te permite **empaquetar y ejecutar aplicaciones en contenedores**.\n",
    "\n",
    "Piensa en un **contenedor** como una **caja de envío estandarizada**. No importa lo que pongas dentro (una aplicación de Python, una base de datos, un servidor web), la caja tiene la misma forma por fuera. Esto significa que puedes mover y ejecutar esa caja en cualquier lugar que entienda el estándar (cualquier máquina con Docker), y lo que está adentro funcionará exactamente igual.\n",
    "\n",
    "Esto resuelve el clásico problema de \"en mi máquina sí funciona\", asegurando que el entorno de desarrollo sea idéntico al de producción.\n",
    "\n",
    "### ¿Qué es un Contenedor?\n",
    "\n",
    "Un **contenedor** es un paquete ligero y ejecutable que incluye todo lo necesario para que una aplicación se ejecute: el código, las librerías, las herramientas del sistema y las dependencias.\n",
    "\n",
    "La diferencia clave con una máquina virtual (VM) es que los contenedores **comparten el kernel del sistema operativo anfitrión**, mientras que una VM incluye un sistema operativo completo. Esto los hace mucho más **ligeros, rápidos y eficientes**.\n",
    "\n",
    "  - **Máquina Virtual**: Es como construir una casa entera (con cimientos, paredes, techo) para cada aplicación.\n",
    "  - **Contenedor**: Es como alquilar un apartamento en un edificio ya existente. Todos los apartamentos (contenedores) comparten los cimientos (el kernel del SO), pero están completamente aislados unos de otros.\n",
    "\n",
    "### El Flujo de Trabajo de Docker\n",
    "\n",
    "El proceso se basa en dos conceptos claves: **Imágenes** y **Contenedores**.\n",
    "\n",
    "**Dockerfile: La Receta**\n",
    "\n",
    "Todo comienza con un `Dockerfile`, que es un archivo de texto con instrucciones paso a paso para construir el entorno de tu aplicación. Es como la receta para hornear un pastel.\n",
    "\n",
    "```dockerfile\n",
    "# Usar una imagen base oficial de Python\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Establecer el directorio de trabajo dentro del contenedor\n",
    "WORKDIR /app\n",
    "\n",
    "# Copiar los archivos de dependencias\n",
    "COPY pyproject.toml uv.lock ./\n",
    "\n",
    "# Instalar las dependencias\n",
    "RUN pip install uv && uv sync uv.lock\n",
    "\n",
    "# Copiar el resto del código de la aplicación\n",
    "COPY ./src /app/src\n",
    "\n",
    "# Comando para ejecutar la aplicación cuando el contenedor se inicie\n",
    "CMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n",
    "```\n",
    "\n",
    "\n",
    "**Imagen Docker: El Pastel Pre-hecho**\n",
    "\n",
    "Cuando ejecutas el comando `docker build` usando un `Dockerfile`, Docker sigue las instrucciones y crea una **Imagen Docker**. Una imagen es una **plantilla inmutable y de solo lectura**. Es como el pastel ya horneado y empaquetado, listo para ser distribuido. Estas imágenes se pueden subir a un registro como Docker Hub para compartirlas.\n",
    "\n",
    "**Contenedor Docker: Comiéndose el Pastel**\n",
    "\n",
    "Finalmente, para ejecutar tu aplicación, creas un **Contenedor** a partir de la imagen con el comando `docker run`. Un contenedor es una **instancia en ejecución de una imagen**. Puedes crear tantos contenedores como quieras a partir de la misma imagen, al igual que puedes repartir muchas porciones del mismo pastel.\n",
    "\n",
    "### Comandos Esenciales de Docker\n",
    "\n",
    "Para trabajar con Docker, usarás un puñado de comandos clave en tu terminal. Aquí están los más importantes:\n",
    "\n",
    "- **`docker build -t mi-aplicacion .`**\n",
    "  Construye una **imagen** a partir de un `Dockerfile` en el directorio actual (`.`). La `-t` le pone un \"tag\" o nombre (`mi-aplicacion`) para que puedas encontrarla fácilmente.\n",
    "\n",
    "- **`docker run -p 8000:80 mi-aplicacion`**\n",
    "  Crea y ejecuta un **contenedor** a partir de la imagen `mi-aplicacion`. El `-p 8000:80` es clave: **mapea el puerto** 8000 de tu máquina al puerto 80 dentro del contenedor, permitiéndote acceder a la aplicación desde tu navegador en `http://localhost:8000`.\n",
    "\n",
    "- **`docker ps`**\n",
    "  Muestra una lista de todos los **contenedores que están en ejecución**. (`ps` viene de \"processes\"). Si añades `-a`, te mostrará todos los contenedores, incluso los que están detenidos.\n",
    "\n",
    "- **`docker stop <id_del_contenedor>`**\n",
    "  Detiene un contenedor en ejecución de forma segura.\n",
    "\n",
    "- **`docker rm <id_del_contenedor>`**\n",
    "  Elimina un contenedor que ya ha sido detenido.\n",
    "\n",
    "- **`docker images`**\n",
    "  Muestra todas las **imágenes** que tienes descargadas en tu máquina.\n",
    "\n",
    "- **`docker rmi <id_de_la_imagen>`**\n",
    "  Elimina una imagen de tu máquina.\n",
    "\n",
    "- **`docker run -it`**\n",
    "\n",
    "  Cuando ejecutas un contenedor, a veces no solo quieres que corra en segundo plano, sino que necesitas **interactuar con él** a través de una terminal, como si estuvieras \"dentro\" del contenedor. Para esto se combinan las banderas `-i` y `-t`.\n",
    "    \n",
    "  **`-i` (`--interactive`)**: Mantiene la entrada estándar (STDIN) abierta. Esto significa que el contenedor puede **recibir** lo que escribes en tu teclado.\n",
    "  **`-t` (`--tty`)**: Asigna una \"pseudo-TTY\" o terminal. Esto le da al contenedor una **interfaz de terminal** para que pueda mostrarte la salida de forma legible.\n",
    "    \n",
    "  Juntos, `-it`, te dan un **shell interactivo** dentro del contenedor. Es perfecto para depurar, explorar el sistema de archivos del contenedor o ejecutar comandos manualmente.\n",
    "\n",
    "  **Ejemplo Práctico**: Iniciar un shell en un contenedor con Ubuntu.\n",
    "    \n",
    "  ```bash\n",
    "      # Descarga la imagen de Ubuntu y te da una terminal de bash dentro\n",
    "      docker run -it ubuntu bash\n",
    "  ```\n",
    "  \n",
    "   Una vez que ejecutas esto, tu terminal cambiará y estarás \"dentro\" del contenedor, donde podrás ejecutar comandos como `ls`, `pwd`, o `apt-get install`.\n",
    "\n",
    "### Volúmenes\n",
    "\n",
    "Por defecto, los contenedores son **efímeros**. Si eliminas un contenedor de base de datos, ¡todos los datos se van con él! Para solucionar esto, se usan los **volúmenes**.\n",
    "\n",
    "Un **volumen** es como una \"mochila\" de almacenamiento que conectas a tu contenedor. Es un directorio en tu máquina anfitriona que se sincroniza con un directorio dentro del contenedor. De esta forma, los datos importantes (como los de una base de datos) se guardan de forma segura en tu máquina, incluso si el contenedor se detiene o se elimina.\n",
    "\n",
    "### Orquestando Múltiples Contenedores con Docker Compose\n",
    "\n",
    "Una aplicación real rara vez es un solo contenedor. Normalmente tienes varios servicios que necesitan comunicarse entre sí: un contenedor para tu API de Python, otro para una base de datos PostgreSQL, y quizás otro para un sistema de caché como Redis.\n",
    "\n",
    "Gestionar todo esto con comandos `docker run` individuales sería un caos. Para eso existe **Docker Compose**.\n",
    "\n",
    "**Docker Compose** es una herramienta que te permite definir y ejecutar aplicaciones multi-contenedor usando un solo archivo de configuración llamado `docker-compose.yml`.  En este archivo, describes todos tus servicios, cómo se conectan entre sí, qué volúmenes usan y qué puertos exponen.\n",
    "\n",
    "Luego, con un solo comando (`docker-compose up`), levantas toda tu aplicación, y con (`docker-compose down`), la detienes y eliminas todo de forma limpia. Es el estándar de facto para el desarrollo local con Docker.\n",
    "\n",
    "#### Estructura de `docker-compose.yml`\n",
    "\n",
    "El archivo `docker-compose.yml` es el **plano de construcción** de tu aplicación multi-contenedor. Utiliza el formato YAML, que es muy legible.\n",
    "\n",
    "Un archivo típico tiene las siguientes secciones principales:\n",
    "\n",
    "  - **`services`**: Es el corazón del archivo. Aquí defines cada uno de los contenedores (llamados \"servicios\") que componen tu aplicación.\n",
    "  - **`volumes`**: Aquí puedes definir \"volúmenes con nombre\" para que tus datos persistan.\n",
    "  - **`networks`**: Permite configurar redes personalizadas para que tus contenedores se comuniquen de forma aislada.\n",
    "\n",
    "#### Desglose de un Servicio\n",
    "\n",
    "Dentro de la sección `services`, cada servicio que defines (por ejemplo, `api` o `db`) tiene una serie de claves para configurarlo:\n",
    "\n",
    "  - **`image`**: Especifica la imagen de Docker Hub que se usará (ej. `postgres:15`).\n",
    "  - **`build`**: Si tienes un `Dockerfile`, aquí indicas la ruta para que Docker Compose construya la imagen por ti (ej. `build: .`).\n",
    "  - **`container_name`**: Un nombre personalizado para el contenedor.\n",
    "  - **`ports`**: Mapea los puertos. La sintaxis es `\"PUERTO_HOST:PUERTO_CONTENEDOR\"`.\n",
    "  - **`environment`**: Define variables de entorno, ideal para pasar configuraciones o secretos como contraseñas de bases de datos.\n",
    "  - **`volumes`**: Conecta los volúmenes. Puede ser un volumen con nombre o una ruta de tu máquina (`./mi-codigo:/app`).\n",
    "  - **`depends_on`**: Le dice a Docker Compose que un servicio depende de otro. Por ejemplo, tu API no debería iniciar hasta que la base de datos esté lista.\n",
    "\n",
    "#### Ejemplo Completo\n",
    "\n",
    "```yaml\n",
    "# Versión del formato del archivo (opcional en versiones recientes)\n",
    "version: '3.8'\n",
    "\n",
    "# Definición de todos los servicios (contenedores)\n",
    "services:\n",
    "  # Nuestro primer servicio: la API de Python\n",
    "  api:\n",
    "    build: . # Construye la imagen usando el Dockerfile en el directorio actual\n",
    "    container_name: mi_api_python\n",
    "    ports:\n",
    "      - \"8000:80\" # Mapea el puerto 8000 de mi máquina al 80 del contenedor\n",
    "    volumes:\n",
    "      - ./src:/app/src # Sincroniza el código fuente para desarrollo en vivo\n",
    "    environment:\n",
    "      - DATABASE_URL=postgresql://user:password@db:5432/mydatabase\n",
    "    depends_on:\n",
    "      - db # Le dice a Docker que no inicie 'api' hasta que 'db' esté listo\n",
    "\n",
    "  # Nuestro segundo servicio: la base de datos PostgreSQL\n",
    "  db:\n",
    "    image: postgres:15-alpine # Usa una imagen oficial de PostgreSQL\n",
    "    container_name: mi_base_de_datos\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    environment:\n",
    "      - POSTGRES_USER=user\n",
    "      - POSTGRES_PASSWORD=password\n",
    "      - POSTGRES_DB=mydatabase\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data # Usa un volumen con nombre para guardar los datos\n",
    "\n",
    "# Definición de los volúmenes con nombre\n",
    "volumes:\n",
    "  postgres_data: # Este volumen persistirá aunque eliminemos el contenedor 'db'\n",
    "\n",
    "```\n",
    "\n",
    "### Docker Hub\n",
    "\n",
    "No siempre tienes que crear tus imágenes desde cero. **Docker Hub** es un registro público (una biblioteca gigante) donde la comunidad y las empresas publican imágenes oficiales y pre-configuradas.\n",
    "\n",
    "Cuando en tu `Dockerfile` escribes `FROM python:3.11-slim`, Docker va a Docker Hub, descarga la imagen oficial de Python y la usa como base. De la misma manera, puedes encontrar imágenes para `postgres`, `nginx`, `mongo`, y casi cualquier tecnología que se te ocurra, lo que acelera enormemente el desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070cd044-6415-44bd-9968-f6b75acee71d",
   "metadata": {},
   "source": [
    "### Dockerfile Optimizado\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile multi-stage optimizado para FastAPI + ML con uv\n",
    "\n",
    "# Etapa 1: Builder - Instalar dependencias\n",
    "FROM python:3.12-slim as builder\n",
    "\n",
    "# Instalar uv (gestor de paquetes rápido)\n",
    "COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\n",
    "\n",
    "# Variables de entorno para optimización\n",
    "ENV UV_COMPILE_BYTECODE=1\n",
    "ENV UV_LINK_MODE=copy\n",
    "ENV PYTHONPATH=/app\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "\n",
    "# Directorio de trabajo\n",
    "WORKDIR /app\n",
    "\n",
    "# Copiar archivos de dependencias\n",
    "COPY pyproject.toml ./\n",
    "\n",
    "# Crear entorno virtual e instalar dependencias\n",
    "RUN uv venv /opt/venv\n",
    "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
    "RUN uv pip install -r pyproject.toml\n",
    "\n",
    "# Etapa 2: Runtime - Aplicación final\n",
    "FROM python:3.12-slim as runtime\n",
    "\n",
    "# Instalar dependencias del sistema necesarias para ML\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    libgomp1 \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Variables de entorno\n",
    "ENV PYTHONPATH=/app\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
    "\n",
    "# Crear usuario no-root para seguridad\n",
    "RUN groupadd -r appuser && useradd -r -g appuser appuser\n",
    "\n",
    "# Directorio de trabajo\n",
    "WORKDIR /app\n",
    "\n",
    "# Copiar entorno virtual desde builder\n",
    "COPY --from=builder /opt/venv /opt/venv\n",
    "\n",
    "# Copiar código de la aplicación\n",
    "COPY src/ /app/src/\n",
    "COPY models/ /app/models/\n",
    "\n",
    "# Crear directorio para logs\n",
    "RUN mkdir -p /app/logs && chown -R appuser:appuser /app\n",
    "\n",
    "# Cambiar a usuario no-root\n",
    "USER appuser\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n",
    "    CMD python -c \"import requests; requests.get('http://localhost:8000/health')\"\n",
    "\n",
    "# Puerto de la aplicación\n",
    "EXPOSE 8000\n",
    "\n",
    "# Comando por defecto - usar FastAPI CLI\n",
    "CMD [\"fastapi\", \"run\", \"src/main.py\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "### Docker Compose para Desarrollo\n",
    "\n",
    "```yaml\n",
    "# docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  # Servicio principal de la API\n",
    "  churn-api:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: churn-prediction-api\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - ENVIRONMENT=development\n",
    "      - LOG_LEVEL=INFO\n",
    "      - MODELS_PATH=/app/models\n",
    "    volumes:\n",
    "      # Volumen para desarrollo - hot reload\n",
    "      - ./src:/app/src:ro\n",
    "      - ./models:/app/models:ro\n",
    "      - ./logs:/app/logs\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 30s\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # Redis para caché (opcional)\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    container_name: churn-api-cache\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    networks:\n",
    "      - ml-network\n",
    "    restart: unless-stopped\n",
    "    command: redis-server --appendonly yes\n",
    "\n",
    "volumes:\n",
    "  redis_data:\n",
    "\n",
    "networks:\n",
    "  ml-network:\n",
    "    driver: bridge\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2aeaa4-5f3a-49ef-86b7-6df0a5ad2124",
   "metadata": {},
   "source": [
    "## 7. Despliegue en la Nube con Fly.io\n",
    "\n",
    "Fly.io es una moderna plataforma en la nube que facilita **desplegar aplicaciones y bases de datos cerca de tus usuarios**, sin importar en qué parte del mundo se encuentren.\n",
    "\n",
    "Piensa en Fly.io como una **aerolínea de carga para tu código**. En lugar de tener un único y gran almacén central (como un servidor tradicional en una sola región), Fly.io tiene pequeños centros de distribución (servidores) en ciudades de todo el mundo. Cuando despliegas tu aplicación, la empaquetan en un contenedor y la envían a las ubicaciones más cercanas a tus usuarios, lo que resulta en una latencia muy baja y una experiencia mucho más rápida.\n",
    "\n",
    "### Despliegue en el \"Edge\"\n",
    "\n",
    "El superpoder de Fly.io es el **despliegue en el borde (edge)**. En lugar de ejecutar tu aplicación en un centro de datos masivo en Virginia (EE. UU.), Fly.io puede ejecutarla simultáneamente en Santiago, São Paulo, Madrid y Tokio.  Cuando un usuario de Chile visita tu sitio, se conecta al servidor de Santiago, no al de EE. UU., lo que reduce drásticamente el tiempo de respuesta.\n",
    "\n",
    "Fly.io logra esto tomando tu aplicación (empaquetada como un contenedor Docker) y ejecutándola en sus propias máquinas virtuales ligeras llamadas **Firecracker**.\n",
    "\n",
    "### El Flujo de Trabajo Típico\n",
    "\n",
    "Desplegar en Fly.io es un proceso muy sencillo que se realiza desde la terminal con su herramienta `flyctl`.\n",
    "\n",
    "1.  **Instalar `flyctl`**: Primero, instalas su interfaz de línea de comandos (CLI) en tu máquina.\n",
    "2.  **`fly launch`**: Navegas al directorio de tu proyecto y ejecutas este comando. `flyctl` es inteligente:\n",
    "    -   **Inspecciona tu código**: Detecta qué tipo de aplicación tienes (Python/FastAPI, Node.js, etc.).\n",
    "    -   **Genera la configuración**: Crea un archivo `fly.toml`, que es el \"manual de instrucciones\" de tu aplicación para la plataforma Fly.io.\n",
    "    -   **Primer despliegue**: Empaqueta tu aplicación en un contenedor Docker y la despliega en la nube.\n",
    "3.  **`fly deploy`**: Una vez que haces cambios en tu código, simplemente ejecutas este comando para redesplegar la nueva versión.\n",
    "4.  **Monitoreo**: Usas comandos como `fly logs` para ver los registros de tu aplicación en tiempo real o `fly status` para comprobar su estado.\n",
    "\n",
    "### El Archivo de Configuración: `fly.toml`\n",
    "\n",
    "Este archivo es el centro de control de tu despliegue. Define todo lo que Fly.io necesita saber:\n",
    "\n",
    "-   El nombre de tu aplicación (`app`).\n",
    "-   Cómo construir la imagen de Docker (`[build]`).\n",
    "-   Qué puertos necesita exponer tu aplicación al mundo exterior (`[[services]]`).\n",
    "-   Verificaciones de estado (`[checks]`) para asegurarse de que tu aplicación funciona correctamente.\n",
    "-   Variables de entorno y secretos.\n",
    "\n",
    "Aunque `fly launch` lo genera automáticamente, puedes editarlo para personalizar completamente tu despliegue.\n",
    "\n",
    "### ¿Por qué es tan popular?\n",
    "\n",
    "-   **Simplicidad**: El flujo de trabajo con `flyctl` es extremadamente directo y fácil de aprender.\n",
    "-   **Rendimiento**: El despliegue en el borde ofrece una latencia muy baja para usuarios de todo el mundo.\n",
    "-   **Plan Gratuito Generoso**: Fly.io ofrece un plan gratuito que es más que suficiente para muchos proyectos personales, prototipos y aplicaciones pequeñas, permitiéndote desplegar hasta 3 aplicaciones pequeñas y bases de datos Postgres sin costo.\n",
    "-   **Bases de Datos**: Facilita enormemente el despliegue y la gestión de bases de datos **PostgreSQL**, que se pueden conectar fácilmente a tu aplicación.\n",
    "\n",
    "### Máquinas Virtuales Firecracker\n",
    "\n",
    "En lugar de usar contenedores Docker directamente en un servidor compartido, Fly.io ejecuta cada contenedor dentro de su propia **Micro Máquina Virtual (MicroVM) Firecracker**.\n",
    "\n",
    "**Firecracker** es una tecnología de virtualización de código abierto desarrollada por Amazon (usada en AWS Lambda). Piensa en ellas como un punto intermedio perfecto entre un contenedor y una máquina virtual tradicional:\n",
    "\n",
    "-   **Arranque ultrarrápido**: Se inician en milisegundos, casi tan rápido como un contenedor.\n",
    "-   **Aislamiento de hardware**: Ofrecen la seguridad y el aislamiento de una máquina virtual completa. Tu aplicación está completamente separada de las demás, lo que es mucho más seguro.\n",
    "-   **Ligeras**: Consumen muy pocos recursos.\n",
    "\n",
    "Este enfoque le da a Fly.io la velocidad de los contenedores con la seguridad de las máquinas virtuales.\n",
    "\n",
    "### \"Apps\" vs. \"Machines\"\n",
    "\n",
    "En la terminología de Fly.io:\n",
    "-   Una **\"App\"** es tu proyecto completo (por ejemplo, `mi-api-python`). Es una agrupación lógica.\n",
    "-   Una **\"Machine\"** es una instancia individual de tu contenedor corriendo en una MicroVM Firecracker.\n",
    "\n",
    "Cuando despliegas tu aplicación en varias regiones (por ejemplo, Chile, Brasil y España), en realidad estás ejecutando tres **\"Machines\"** idénticas, todas parte de la misma **\"App\"**. Fly.io se encarga de dirigir a tus usuarios a la \"Machine\" más cercana.\n",
    "\n",
    "### Almacenamiento Persistente con Volúmenes\n",
    "\n",
    "Para aplicaciones que necesitan guardar datos (como una base de datos), Fly.io ofrece **volúmenes de almacenamiento**. Estos son discos duros virtuales **NVMe** de alta velocidad que se adjuntan a tus Machines.\n",
    "\n",
    "Un punto clave es que un volumen está **atado a una región específica**. Si creas un volumen para tu base de datos en Santiago (Chile), esa base de datos principal vivirá allí. Puedes crear réplicas de lectura en otras regiones para acelerar las consultas, pero la escritura principal se hará en la región del volumen.\n",
    "\n",
    "### Red Privada Segura (Private Networking)\n",
    "\n",
    "Esta es una de las características más potentes. Cuando despliegas una \"App\", Fly.io crea automáticamente una **red privada y segura** para todas las \"Machines\" que pertenecen a esa aplicación, sin importar en qué parte del mundo se encuentren. \n",
    "\n",
    "Esto significa que tu `api-web` en Madrid puede hablar con tu `base-de-datos` en São Paulo a través de esta red interna, usando direcciones IPv6 privadas, sin exponer nunca la base de datos a la internet pública. Es como si todas tus máquinas estuvieran conectadas por un cable secreto y seguro.\n",
    "\n",
    "### Fly Postgres: Bases de Datos Simplificadas\n",
    "\n",
    "Fly.io tiene un soporte de primera clase para **PostgreSQL**. Con unos pocos comandos `flyctl`, puedes:\n",
    "-   **Crear un clúster de Postgres** de alta disponibilidad en segundos.\n",
    "-   **Asociarlo** a tu aplicación para que se conecten a través de la red privada.\n",
    "-   **Escalarlo** fácilmente, añadiendo réplicas de lectura en otras regiones para acelerar las consultas de tus usuarios globales.\n",
    "\n",
    "Esto elimina gran parte de la complejidad de administrar y escalar bases de datos, permitiéndote concentrarte en tu aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e909b1f-9de6-4921-bbdb-63af64a803d5",
   "metadata": {},
   "source": [
    "### Script de Deployment\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# scripts/deploy-fly.sh\n",
    "\n",
    "# Script de deployment para Fly.io\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"🚀 Iniciando deployment a Fly.io\"\n",
    "\n",
    "# Variables\n",
    "FLY_APP_NAME=\"churn-prediction-api\"\n",
    "REGION=\"mad\"  # Madrid\n",
    "\n",
    "# Verificar que flyctl esté instalado\n",
    "check_flyctl() {\n",
    "    if ! command -v flyctl &> /dev/null; then\n",
    "        echo \"❌ flyctl no está instalado\"\n",
    "        echo \"Instala flyctl desde: https://fly.io/docs/hands-on/install-flyctl/\"\n",
    "        exit 1\n",
    "    fi\n",
    "    echo \"✅ flyctl instalado\"\n",
    "}\n",
    "\n",
    "# Verificar autenticación\n",
    "check_auth() {\n",
    "    if ! flyctl auth whoami &> /dev/null; then\n",
    "        echo \"⚠️ No estás autenticado en Fly.io\"\n",
    "        echo \"Ejecutando 'flyctl auth login'...\"\n",
    "        flyctl auth login\n",
    "    fi\n",
    "    echo \"✅ Autenticado en Fly.io\"\n",
    "}\n",
    "\n",
    "# Verificar que los modelos existen\n",
    "check_models() {\n",
    "    if [ ! -d \"models\" ] || [ -z \"$(ls -A models/*.joblib 2>/dev/null)\" ]; then\n",
    "        echo \"❌ No se encontraron modelos entrenados\"\n",
    "        exit 1\n",
    "    fi\n",
    "    echo \"✅ Modelos encontrados\"\n",
    "}\n",
    "\n",
    "# Crear aplicación si no existe\n",
    "create_or_update_app() {\n",
    "    if flyctl apps show $FLY_APP_NAME &> /dev/null; then\n",
    "        echo \"✅ Aplicación '$FLY_APP_NAME' ya existe\"\n",
    "    else\n",
    "        echo \"📱 Creando nueva aplicación...\"\n",
    "        flyctl apps create $FLY_APP_NAME --region $REGION\n",
    "        echo \"✅ Aplicación creada\"\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Deploy de la aplicación\n",
    "deploy_app() {\n",
    "    echo \"🚀 Iniciando deployment...\"\n",
    "    flyctl deploy --remote-only --strategy immediate\n",
    "    echo \"✅ Deployment completado\"\n",
    "}\n",
    "\n",
    "# Verificar que el deployment funcionó\n",
    "verify_deployment() {\n",
    "    local app_url=\"https://${FLY_APP_NAME}.fly.dev\"\n",
    "    \n",
    "    echo \"🔍 Verificando deployment...\"\n",
    "    sleep 10\n",
    "    \n",
    "    if curl -f \"${app_url}/health\" > /dev/null 2>&1; then\n",
    "        echo \"✅ ¡Deployment exitoso!\"\n",
    "        echo \"📖 Documentación API: ${app_url}/docs\"\n",
    "        echo \"🔍 Health check: ${app_url}/health\"\n",
    "        return 0\n",
    "    else\n",
    "        echo \"❌ Deployment falló\"\n",
    "        flyctl logs --app $FLY_APP_NAME\n",
    "        return 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Función principal\n",
    "main() {\n",
    "    echo \"════════════════════════════════════════\"\n",
    "    echo \"   🚀 DEPLOYMENT A FLY.IO\"\n",
    "    echo \"════════════════════════════════════════\"\n",
    "    \n",
    "    check_flyctl\n",
    "    check_auth\n",
    "    check_models\n",
    "    create_or_update_app\n",
    "    deploy_app\n",
    "    \n",
    "    if verify_deployment; then\n",
    "        echo \"🎉 ¡Deployment completado exitosamente!\"\n",
    "    else\n",
    "        echo \"💥 Deployment falló. Revisa los logs.\"\n",
    "        exit 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Ejecutar función principal\n",
    "main \"$@\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e99e4-4c5d-471c-a5a7-a1a67ad45987",
   "metadata": {},
   "source": [
    "## 8. Testing\n",
    "\n",
    "Las **pruebas de software** son actividades diseñadas para verificar que una aplicación cumple con los requisitos esperados y funciona correctamente. Permiten detectar errores, validar comportamientos y garantizar calidad antes de poner el software en producción.\n",
    "\n",
    "### Conceptos clave\n",
    "- **Caso de prueba**: conjunto de condiciones y entradas que verifican un comportamiento específico.\n",
    "- **Resultado esperado**: lo que debería ocurrir al ejecutar el caso de prueba.\n",
    "- **Resultado real**: lo que ocurre efectivamente durante la prueba.\n",
    "- **Bug o defecto**: discrepancia entre el resultado esperado y el real.\n",
    "\n",
    "\n",
    "### ¿Por qué son importantes las Pruebas?\n",
    "\n",
    "Escribir pruebas puede parecer trabajo extra al principio, pero los beneficios son enormes. Son como la **red de seguridad para un trapecista**.\n",
    "\n",
    "  * **Confianza para el cambio (Refactoring):** Te permiten modificar y mejorar tu código con la seguridad de que no romperás algo sin darte cuenta. Si tus pruebas siguen pasando, tus cambios son seguros.\n",
    "  * **Detección temprana de errores:** Atrapas los bugs mucho antes en el ciclo de desarrollo, cuando son más fáciles y baratos de arreglar.\n",
    "  * **Documentación viva:** Las pruebas son el mejor ejemplo de cómo se debe usar tu código. Un nuevo desarrollador puede leer las pruebas para entender la funcionalidad.\n",
    "  * **Despliegues sin miedo:** Automatizar tus pruebas te da la confianza para lanzar nuevas versiones de tu software sabiendo que las funcionalidades clave han sido verificadas.\n",
    "\n",
    "\n",
    "### Tipos Comunes de Pruebas (La Pirámide de Pruebas)\n",
    "\n",
    "No todas las pruebas son iguales. Se suelen clasificar según lo que abarcan:\n",
    "\n",
    "  * **Pruebas Unitarias (Unit Tests):** Son la base de la pirámide. Se enfocan en la pieza más pequeña de código posible (una sola función o método) de forma **aislada**. Si una función suma dos números, la prueba unitaria solo verifica eso, sin preocuparse por la base de datos o la interfaz de usuario. Son **muy rápidas** de ejecutar.\n",
    "\n",
    "  * **Pruebas de Integración (Integration Tests):** Están en el medio. Verifican que **varias partes de tu sistema funcionan bien juntas**. Por ejemplo, ¿puede tu código escribir correctamente en la base de datos? ¿Se comunica bien tu aplicación con una API externa? Son más lentas que las unitarias.\n",
    "\n",
    "  * **Pruebas End-to-End (E2E):** Están en la cima. Simulan el **flujo completo de un usuario real** a través de toda la aplicación. Por ejemplo, una prueba E2E podría automatizar un navegador para que inicie sesión, agregue un producto al carrito y complete la compra. Son las más **lentas y complejas**, pero verifican que todo el sistema funciona como un todo.\n",
    "\n",
    "Dentro de este universo, se encuentran las **pruebas de API**, que abordaremos a continuación.\n",
    "\n",
    "### Pruebas de API\n",
    "\n",
    "Imagina una API (Interfaz de Programación de Aplicaciones) como el **mesero de un restaurante**. Tú (el cliente o *frontend*) le pides algo al mesero (la **API**), él lleva tu pedido a la cocina (el servidor o *backend*), y te trae de vuelta tu plato (los **datos**).\n",
    "\n",
    "Las **pruebas de API** consisten en enviar solicitudes a una API (generalmente REST o GraphQL) y verificar que las respuestas:\n",
    "- Tengan el **código de estado HTTP** esperado.\n",
    "- Devuelvan la **estructura y datos correctos** (JSON, XML, etc.).\n",
    "- Cumplan con las **reglas de negocio**.\n",
    "\n",
    "Ejemplo: verificar que al consultar un usuario, la API retorne su nombre, email y estado correcto.\n",
    "\n",
    "La librería más popular para esto en Python es **`requests`**. Te permite hacer solicitudes HTTP (GET, POST, PUT, DELETE) de forma muy simple.\n",
    "\n",
    "**Ejemplo simple con `requests`:**\n",
    "\n",
    "Este código hace una petición a una API pública de prueba y verifica que la respuesta sea exitosa (código 200) y que el `id` del usuario sea el que pedimos.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# 1. Definir la URL del endpoint de la API\n",
    "api_url = \"https://jsonplaceholder.typicode.com/users/1\"\n",
    "\n",
    "# 2. Hacer la solicitud GET a la API\n",
    "response = requests.get(api_url)\n",
    "\n",
    "# 3. Verificar los resultados\n",
    "# Verificar que la solicitud fue exitosa (código 200 OK)\n",
    "assert response.status_code == 200\n",
    "\n",
    "# Convertir la respuesta a formato JSON (un diccionario de Python)\n",
    "user_data = response.json()\n",
    "\n",
    "# Verificar que los datos recibidos son correctos\n",
    "assert user_data['id'] == 1\n",
    "assert user_data['name'] == \"Leanne Graham\"\n",
    "\n",
    "print(\"✅ ¡El test de la API pasó correctamente!\")\n",
    "\n",
    "```\n",
    "\n",
    "### ¿Por qué automatizar pruebas de API?\n",
    "\n",
    "- **Rapidez**: ejecutar pruebas automáticamente después de cada cambio.\n",
    "- **Confiabilidad**: evita errores humanos en pruebas manuales.\n",
    "- **Repetibilidad**: se pueden ejecutar en CI/CD.\n",
    "- **Cobertura**: validan múltiples casos (errores, respuestas válidas, límites, etc.).\n",
    "\n",
    "### Buenas prácticas en pruebas de API\n",
    "\n",
    "- Usar **fixtures de pytest** para inicializar configuraciones.\n",
    "- Separar pruebas por **módulos** (ej: `test_users.py`, `test_posts.py`).\n",
    "- Incluir **tests positivos y negativos**.\n",
    "- Integrar con CI/CD (GitHub Actions, GitLab CI, etc.).\n",
    "\n",
    "### Tests Automatizados con Pytest\n",
    "\n",
    "**Pytest** es un *framework* que hace que escribir, organizar y ejecutar tests en Python sea increíblemente fácil y potente. Su objetivo es que puedas enfocarte en qué probar, no en cómo hacerlo.\n",
    "\n",
    "Piensa en Pytest como el **gerente del restaurante**. En lugar de que tú vayas a la cocina a verificar cada plato, le das al gerente una lista de \"verificaciones\" y él se encarga de ejecutarlas todas, cada vez que hay un cambio en el menú, y te entrega un informe detallado de lo que está bien y lo que está mal.\n",
    "\n",
    "### ¿Por qué usar Pytest?\n",
    "\n",
    "  * **Sintaxis Sencilla:** Usa `assert` directamente, lo que hace el código muy legible. No necesitas aprender comandos complejos.\n",
    "  * **Detección Automática:** Simplemente nombra tus archivos como `test_*.py` y tus funciones de prueba como `test_*()`, y Pytest los encontrará y ejecutará automáticamente.\n",
    "  * **Fixtures:** Son funciones de ayuda que preparan el \"escenario\" para tus tests (ej: conectarse a una base de datos, crear un usuario de prueba) y lo limpian después. Son reutilizables y muy potentes.\n",
    "  * **Informes Detallados:** Si un test falla, Pytest te dice exactamente en qué línea y por qué, comparando los valores esperados con los obtenidos.\n",
    "  * **Gran Ecosistema:** Tiene cientos de *plugins* para extender su funcionalidad (ej: reportes de cobertura, ejecución en paralelo, etc.).\n",
    "\n",
    "### Estructura básica de un test\n",
    "\n",
    "Un test en `pytest` es simplemente una función cuyo nombre empieza con `test_`:\n",
    "\n",
    "```python\n",
    "def test_suma():\n",
    "    assert 1 + 1 == 2\n",
    "```\n",
    "\n",
    "### Juntando todo: Testing de API con Pytest\n",
    "\n",
    "Ahora, combinemos los dos conceptos. Usamos la librería `requests` para comunicarnos con la API y `pytest` para estructurar y automatizar las pruebas. Vamos a probar la API gratuita de [JSONPlaceholder](https://jsonplaceholder.typicode.com/) que simula datos falsos.\n",
    "\n",
    "Primero, instala las librerías si no las tienes:\n",
    "\n",
    "```bash\n",
    "uv add pytest requests\n",
    "```\n",
    "\n",
    "Luego, crea un archivo llamado `test_api.py`:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# La URL base de la API que vamos a probar\n",
    "BASE_URL = \"https://jsonplaceholder.typicode.com\"\n",
    "\n",
    "def test_get_user_by_id():\n",
    "    \"\"\"\n",
    "    Prueba que se puede obtener un usuario por su ID y que sus datos son correctos.\n",
    "    \"\"\"\n",
    "    # Hacer la petición a un endpoint específico\n",
    "    response = requests.get(f\"{BASE_URL}/users/1\")\n",
    "    \n",
    "    # Verificar el código de estado\n",
    "    assert response.status_code == 200, \"El código de estado debería ser 200 OK\"\n",
    "    \n",
    "    # Verificar el contenido de la respuesta\n",
    "    user_data = response.json()\n",
    "    assert user_data['id'] == 1\n",
    "    assert user_data['email'] == \"Sincere@april.biz\"\n",
    "\n",
    "def test_get_all_posts():\n",
    "    \"\"\"\n",
    "    Prueba que se puede obtener una lista de posts y que no está vacía.\n",
    "    \"\"\"\n",
    "    response = requests.get(f\"{BASE_URL}/posts\")\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    \n",
    "    posts = response.json()\n",
    "    # Verificar que la respuesta es una lista y contiene 100 posts\n",
    "    assert isinstance(posts, list)\n",
    "    assert len(posts) == 100\n",
    "\n",
    "```\n",
    "\n",
    "Para ejecutar estas pruebas, simplemente abre tu terminal en la carpeta donde guardaste el archivo y ejecuta el comando:\n",
    "\n",
    "```bash\n",
    "pytest\n",
    "```\n",
    "\n",
    "Pytest encontrará y ejecutará automáticamente las funciones `test_get_user_by_id` y `test_get_all_posts`, y te mostrará un resumen claro de los resultados.\n",
    "\n",
    "\n",
    "### Manejo de escenarios negativos\n",
    "\n",
    "También es importante probar **casos de error**:\n",
    "\n",
    "```python\n",
    "def test_user_not_found():\n",
    "    response = requests.get(f\"{BASE_URL}/users/9999\")\n",
    "    assert response.status_code == 404\n",
    "```\n",
    "\n",
    "### La Anatomía de un Buen Test\n",
    "\n",
    "Una excelente práctica para que tus pruebas sean claras y legibles es seguir el patrón **AAA**:\n",
    "\n",
    "1.  **Arrange (Preparar):** Configura todo lo necesario para la prueba. Define las variables, crea objetos, prepara el fixture.\n",
    "2.  **Act (Actuar):** Ejecuta la única pieza de código que quieres probar. Llama a tu función o método.\n",
    "3.  **Assert (Verificar):** Comprueba que el resultado de la acción es el esperado.\n",
    "\n",
    "\n",
    "```python\n",
    "def test_convertir_a_mayusculas():\n",
    "    # 1. Arrange (Preparar)\n",
    "    texto_original = \"hola mundo\"\n",
    "    \n",
    "    # 2. Act (Actuar)\n",
    "    texto_convertido = texto_original.upper()\n",
    "    \n",
    "    # 3. Assert (Verificar)\n",
    "    assert texto_convertido == \"HOLA MUNDO\"\n",
    "```\n",
    "\n",
    "Este patrón hace que sea muy fácil entender qué está probando cada test.\n",
    "\n",
    "\n",
    "### Profundizando en Pytest\n",
    "\n",
    "Ya vimos lo básico, pero aquí es donde Pytest realmente brilla.\n",
    "\n",
    "**Fixtures:**\n",
    "\n",
    "Los **fixtures** son el concepto más poderoso de Pytest. Son funciones que preparan el \"escenario\" para tus pruebas (setup) y lo limpian después (teardown). Son reutilizables y se inyectan en tus pruebas simplemente pasándolos como argumentos.\n",
    "\n",
    "**Ejemplo:** Un fixture para crear un archivo temporal antes de una prueba y asegurarse de que se borre después, aunque la prueba falle.\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "import os\n",
    "\n",
    "# Define el fixture\n",
    "@pytest.fixture\n",
    "def archivo_temporal():\n",
    "    # --- ARRANGE (Setup) ---\n",
    "    print(\"\\nCreando archivo temporal...\")\n",
    "    ruta_archivo = \"test_file.txt\"\n",
    "    with open(ruta_archivo, \"w\") as f:\n",
    "        f.write(\"hola mundo\")\n",
    "    \n",
    "    yield ruta_archivo  # Aquí se ejecuta la prueba que usa el fixture\n",
    "    \n",
    "    # --- TEARDOWN (Cleanup) ---\n",
    "    print(\"\\nBorrando archivo temporal...\")\n",
    "    os.remove(ruta_archivo)\n",
    "\n",
    "# Usa el fixture en una prueba\n",
    "def test_leer_archivo(archivo_temporal):\n",
    "    with open(archivo_temporal, \"r\") as f:\n",
    "        contenido = f.read()\n",
    "    assert contenido == \"hola mundo\"\n",
    "\n",
    "```\n",
    "\n",
    "**Parametrize**\n",
    "\n",
    "A menudo quieres probar la misma función con diferentes entradas y salidas. En lugar de escribir un test para cada caso, puedes usar `@pytest.mark.parametrize`.\n",
    "\n",
    "```python\n",
    "# test_calculadora.py\n",
    "import pytest\n",
    "\n",
    "def sumar(a, b):\n",
    "    return a + b\n",
    "\n",
    "@pytest.mark.parametrize(\"a, b, esperado\", [\n",
    "    (1, 2, 3),        # Caso 1: positivos\n",
    "    (-1, 1, 0),       # Caso 2: con negativos\n",
    "    (5, 0, 5),        # Caso 3: con cero\n",
    "    (-2, -3, -5)      # Caso 4: negativos\n",
    "])\n",
    "def test_suma_multiple(a, b, esperado):\n",
    "    resultado = sumar(a, b)\n",
    "    assert resultado == esperado\n",
    "```\n",
    "\n",
    "Pytest ejecutará `test_suma_multiple` **cuatro veces**, una por cada grupo de datos, dándote un informe detallado de cada caso.\n",
    "\n",
    "**Markers**\n",
    "\n",
    "Los marcadores te permiten categorizar tus pruebas. Puedes crear marcadores personalizados (ej: `@pytest.mark.slow`, `@pytest.mark.api`) y luego decirle a Pytest que ejecute solo las pruebas con una etiqueta específica.\n",
    "\n",
    "```python\n",
    "@pytest.mark.slow\n",
    "def test_proceso_largo():\n",
    "    # ... un test que tarda mucho en ejecutarse\n",
    "\n",
    "@pytest.mark.api\n",
    "def test_conexion_api_externa():\n",
    "    # ... un test que depende de una API\n",
    "```\n",
    "\n",
    "Para ejecutar solo los tests de API, usarías: `pytest -m api`\n",
    "\n",
    "\n",
    "**Mocks**\n",
    "\n",
    "Para no depender de una API externa en cada prueba, podemos usar `unittest.mock` para simular respuestas.\n",
    "\n",
    "```python\n",
    "from unittest.mock import patch, Mock\n",
    "\n",
    "@patch(\"requests.get\")\n",
    "def test_mocked_api(mock_get):\n",
    "    mock_response = Mock()\n",
    "    mock_response.status_code = 200\n",
    "    mock_response.json.return_value = {\"id\": 1, \"name\": \"Test User\"}\n",
    "    mock_get.return_value = mock_response\n",
    "\n",
    "    response = requests.get(\"https://fakeapi.com/users/1\")\n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert data[\"name\"] == \"Test User\"\n",
    "```\n",
    "\n",
    "**Pruebas asíncronas con `pytest-asyncio`**\n",
    "\n",
    "Si la API o cliente HTTP es asíncrono (ej: `httpx`, `aiohttp`), usamos `pytest-asyncio`:\n",
    "\n",
    "Instalación:\n",
    "```bash\n",
    "uv add pytest-asyncio httpx\n",
    "```\n",
    "\n",
    "Ejemplo:\n",
    "```python\n",
    "import pytest\n",
    "import httpx\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_async_api():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(\"https://jsonplaceholder.typicode.com/posts/1\")\n",
    "        assert response.status_code == 200\n",
    "        data = response.json()\n",
    "        assert \"title\" in data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257f8db-0b4a-479b-8c67-b16b44454ccf",
   "metadata": {},
   "source": [
    "### Tests de la App de FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562f81b-e5b3-4655-b63f-f42e9cba5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_api.py\n",
    "import pytest\n",
    "import httpx\n",
    "from fastapi.testclient import TestClient\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configurar path para imports\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n",
    "\n",
    "from main import app\n",
    "\n",
    "# Cliente de testing\n",
    "client = TestClient(app)\n",
    "\n",
    "class TestHealthEndpoints:\n",
    "    \"\"\"Tests para endpoints de salud y monitoreo.\"\"\"\n",
    "    \n",
    "    def test_root_endpoint(self):\n",
    "        \"\"\"Test del endpoint raíz.\"\"\"\n",
    "        response = client.get(\"/\")\n",
    "        assert response.status_code == 200\n",
    "        \n",
    "        data = response.json()\n",
    "        assert data[\"message\"] == \"🎯 Churn Prediction API\"\n",
    "        assert data[\"version\"] == \"1.0.0\"\n",
    "        assert \"status\" in data\n",
    "    \n",
    "    def test_health_check(self):\n",
    "        \"\"\"Test del health check básico.\"\"\"\n",
    "        response = client.get(\"/health\")\n",
    "        assert response.status_code == 200\n",
    "        \n",
    "        data = response.json()\n",
    "        assert data[\"status\"] == \"healthy\"\n",
    "        assert \"models_available\" in data\n",
    "\n",
    "class TestPredictionEndpoints:\n",
    "    \"\"\"Tests para endpoints de predicción.\"\"\"\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def valid_customer_data(self):\n",
    "        \"\"\"Datos válidos de cliente para testing.\"\"\"\n",
    "        return {\n",
    "            \"gender\": \"Female\",\n",
    "            \"senior_citizen\": 0,\n",
    "            \"partner\": \"Yes\",\n",
    "            \"dependents\": \"No\",\n",
    "            \"tenure\": 24,\n",
    "            \"phone_service\": \"Yes\",\n",
    "            \"internet_service\": \"Fiber optic\",\n",
    "            \"online_security\": \"No\",\n",
    "            \"tech_support\": \"Yes\",\n",
    "            \"contract\": \"Month-to-month\",\n",
    "            \"payment_method\": \"Electronic check\",\n",
    "            \"monthly_charges\": 85.50,\n",
    "            \"total_charges\": 2052.00,\n",
    "            \"customer_id\": \"TEST001\"\n",
    "        }\n",
    "    \n",
    "    def test_predict_valid_customer(self, valid_customer_data):\n",
    "        \"\"\"Test de predicción con datos válidos.\"\"\"\n",
    "        response = client.post(\"/predict\", json=valid_customer_data)\n",
    "        assert response.status_code == 200\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Verificar estructura de la respuesta\n",
    "        required_fields = [\n",
    "            \"prediction\", \"churn_probability\", \"retention_probability\",\n",
    "            \"risk_category\", \"confidence\", \"model_info\", \n",
    "            \"processing_time_ms\", \"timestamp\"\n",
    "        ]\n",
    "        \n",
    "        for field in required_fields:\n",
    "            assert field in data, f\"Campo {field} faltante en respuesta\"\n",
    "        \n",
    "        # Verificar tipos y rangos\n",
    "        assert isinstance(data[\"prediction\"], int)\n",
    "        assert data[\"prediction\"] in [0, 1]\n",
    "        \n",
    "        assert 0 <= data[\"churn_probability\"] <= 1\n",
    "        assert 0 <= data[\"retention_probability\"] <= 1\n",
    "        \n",
    "        assert data[\"risk_category\"] in [\"Low\", \"Medium\", \"High\"]\n",
    "        assert data[\"processing_time_ms\"] > 0\n",
    "    \n",
    "    def test_predict_invalid_data(self):\n",
    "        \"\"\"Test con datos inválidos.\"\"\"\n",
    "        invalid_data = {\n",
    "            \"gender\": \"Other\",  # No permitido\n",
    "            \"senior_citizen\": 2,  # Fuera de rango\n",
    "            \"tenure\": -5,  # Negativo\n",
    "            \"monthly_charges\": 0  # Debe ser > 0\n",
    "        }\n",
    "        \n",
    "        response = client.post(\"/predict\", json=invalid_data)\n",
    "        assert response.status_code == 422  # Unprocessable Entity\n",
    "\n",
    "# Script para ejecutar tests\n",
    "if __name__ == \"__main__\":\n",
    "    pytest.main([__file__])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2297c-47af-4aaa-8ad2-9bbae491128b",
   "metadata": {},
   "source": [
    "## 9. Mejores Prácticas y Tips (...)\n",
    "\n",
    "### Checklist de Producción\n",
    "\n",
    "```\n",
    "# 📋 CHECKLIST DE PRODUCCIÓN PARA API DE ML\n",
    "\n",
    "## ✅ Código y Arquitectura\n",
    "- [ ] Código limpio y bien documentado\n",
    "- [ ] Separación clara entre entrenamiento e inferencia\n",
    "- [ ] Pipelines de scikit-learn para consistencia\n",
    "- [ ] Validación robusta con Pydantic\n",
    "- [ ] Manejo de errores comprehensivo\n",
    "- [ ] Logging estructurado configurado\n",
    "- [ ] Tests unitarios e integración (>80% cobertura)\n",
    "\n",
    "## ✅ Modelo y Datos\n",
    "- [ ] Modelo validado en datos de prueba\n",
    "- [ ] Métricas de rendimiento documentadas\n",
    "- [ ] Versionado de modelos implementado\n",
    "- [ ] Backup de modelos configurado\n",
    "\n",
    "## ✅ API y Rendimiento\n",
    "- [ ] Documentación API completa (Swagger)\n",
    "- [ ] Health checks funcionando\n",
    "- [ ] CORS configurado apropiadamente\n",
    "- [ ] Timeouts configurados\n",
    "\n",
    "## ✅ Seguridad\n",
    "- [ ] Variables sensibles en variables de entorno\n",
    "- [ ] Usuario no-root en contenedor Docker\n",
    "- [ ] HTTPS habilitado\n",
    "- [ ] Validación de entrada estricta\n",
    "\n",
    "## ✅ Infraestructura\n",
    "- [ ] Dockerfile optimizado (multi-stage)\n",
    "- [ ] Imagen Docker pequeña y eficiente\n",
    "- [ ] Monitoreo y alertas configurados\n",
    "- [ ] Backup y recuperación probados\n",
    "```\n",
    "\n",
    "### Optimizaciones de Rendimiento\n",
    "\n",
    "```python\n",
    "# Técnicas de optimización para APIs de ML\n",
    "\n",
    "# 1. Cache de modelos\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def load_model_cached(model_path: str):\n",
    "    \"\"\"Cargar modelo con cache para evitar recargas.\"\"\"\n",
    "    return joblib.load(model_path)\n",
    "\n",
    "# 2. Procesamiento por lotes\n",
    "@app.post(\"/predict/batch\")\n",
    "async def batch_predict(requests: List[PredictionRequest]):\n",
    "    \"\"\"Procesar múltiples predicciones eficientemente.\"\"\"\n",
    "    # Preparar datos para predicción vectorizada\n",
    "    batch_data = [req.features.dict() for req in requests]\n",
    "    \n",
    "    # Predicción vectorizada (más eficiente)\n",
    "    predictions = model.predict(batch_data)\n",
    "    probabilities = model.predict_proba(batch_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 3. Async/Await para I/O\n",
    "async def log_prediction_async(prediction_data: dict):\n",
    "    \"\"\"Logging asíncrono para no bloquear requests.\"\"\"\n",
    "    async with aiofiles.open(\"predictions.log\", \"a\") as f:\n",
    "        await f.write(f\"{json.dumps(prediction_data)}\\n\")\n",
    "\n",
    "# 4. Configuración de Uvicorn para producción\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\n",
    "        \"main:app\",\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        workers=4,  # Número de workers basado en CPU\n",
    "        loop=\"uvloop\",  # Loop más rápido\n",
    "        http=\"httptools\",  # Parser HTTP más rápido\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ae230-1d51-4d19-b2f3-c99fd001e4da",
   "metadata": {},
   "source": [
    "## 10. Conclusiones y Próximos Pasos\n",
    "\n",
    "### Resumen de lo Implementado\n",
    "\n",
    "🎓 **Has aprendido a implementar un sistema completo de ML en producción:**\n",
    "\n",
    "1. **Gestión moderna de proyectos** con UV y pyproject.toml\n",
    "2. **Machine Learning pipelines** con scikit-learn\n",
    "3. **API robusta** con FastAPI y validación Pydantic\n",
    "4. **Contenedorización** optimizada con Docker\n",
    "5. **Deployment** en la nube con Fly.io\n",
    "6. **Testing automatizado** con pytest\n",
    "7. **Monitoreo** y observabilidad\n",
    "\n",
    "### Valor Agregado vs. Enfoques Tradicionales\n",
    "\n",
    "| Aspecto | Tradicional (Flask + pip) | Moderno (FastAPI + uv) |\n",
    "|---------|---------------------------|------------------------|\n",
    "| **Velocidad API** | ~1000 req/s | ~3000+ req/s |\n",
    "| **Documentación** | Manual | Automática |\n",
    "| **Validación** | Manual | Automática |\n",
    "| **Install deps** | pip install (30s) | uv sync (3s) |\n",
    "| **Typing** | Opcional | Nativo |\n",
    "| **Async** | Complejo | Nativo |\n",
    "\n",
    "### Próximos Pasos Recomendados\n",
    "\n",
    "🛣️ **Extensiones avanzadas:**\n",
    "\n",
    "1. **MLOps**: Integrar MLflow para model registry\n",
    "2. **Monitoreo**: Añadir Prometheus + Grafana\n",
    "3. **Seguridad**: Implementar autenticación JWT\n",
    "4. **Escalabilidad**: Añadir Redis cache y load balancing\n",
    "5. **CI/CD**: GitHub Actions para deployment automático\n",
    "\n",
    "### Comandos Finales\n",
    "\n",
    "```bash\n",
    "# Configuración inicial\n",
    "uv sync                                 # Instalar dependencias\n",
    "python scripts/setup.py               # Configurar proyecto\n",
    "\n",
    "# Desarrollo local\n",
    "uv run uvicorn src.main:app --reload  # Servidor desarrollo\n",
    "\n",
    "# Testing\n",
    "pytest tests/ --cov=src               # Tests con cobertura\n",
    "\n",
    "# Deployment\n",
    "./scripts/deploy-fly.sh              # Deploy a producción\n",
    "```\n",
    "\n",
    "### Estructura Final del Proyecto\n",
    "\n",
    "```\n",
    "mi-proyecto-ml/\n",
    "├── src/                    # Código fuente\n",
    "│   ├── main.py            # Aplicación FastAPI\n",
    "│   ├── schemas/           # Modelos Pydantic\n",
    "│   └── ml/                # Lógica ML\n",
    "├── models/                # Modelos entrenados (.joblib)\n",
    "├── tests/                 # Tests automatizados\n",
    "├── scripts/               # Scripts de utilidad\n",
    "├── Dockerfile             # Contenedorización\n",
    "├── fly.toml              # Config Fly.io\n",
    "└── pyproject.toml        # Dependencias UV\n",
    "```\n",
    "\n",
    "🎉 **¡Felicitaciones! Has implementado una API de ML moderna, escalable y lista para producción usando las mejores prácticas y herramientas de 2025.**\n",
    "\n",
    "Tu API ahora puede:\n",
    "- ✅ Servir predicciones de ML a miles de usuarios\n",
    "- ✅ Validar datos automáticamente\n",
    "- ✅ Documentarse a sí misma\n",
    "- ✅ Desplegarse globalmente en segundos\n",
    "- ✅ Monitorearse y alertar automáticamente\n",
    "- ✅ Escalarse según demanda\n",
    "\n",
    "**¡Es hora de llevarlo a producción y ver tu modelo en acción!** 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de9431-3b8a-47a3-bab6-29f5000c1a0b",
   "metadata": {},
   "source": [
    "> **Nota**  \n",
    "> Esta notebook se inspiró en el workshop [**mlzoomcamp-fastapi-uv**](https://github.com/alexeygrigorev/workshops/tree/main/mlzoomcamp-fastapi-uv), ofrecido por *Alexey Grigorev*, fundador de **DataTalks.Club**. \n",
    "> Agradecimientos a la comunidad por compartir estos recursos abiertos.\n",
    "> Además, me tomé la molestia de guardar los archivos del workshop en la carpeta **`workshop_fastapi_ml`** para que tengas acceso rápido al material de referencia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
